---
layout: post
title: "240625(화) [온라인강의] Pytorch4_텐서조작(2)"
subtitle: "[Tips]"
date: 2024-06-25 22:21
background: 
tag: [Tips, Github io, Notion]
---

# [온라인강의]Pytorch5_텐서조작(2)

### **~ 목차 ~**
0. PyTorch 설치 및 불러오기   
1. 텐서 연산 및 조작   
  1.1 텐서 간의 계산 실습   
  1.2 Broadcasting을 이용한 텐서값 변경   
  1.3 Broadcasting을 이용한 차원이 다른 텐서 간의 계산 실습   
2. Sparse Tensor 조작 및 실습   
  2.1 COO Tensor에 대한 이해 및 실습   
  2.2 CSC/CSR Tensor에 대한 이해 및 실습   
  2.3 Sparse Tensor의 필요성 이해 및 실습   
  2.4 Sparse Tensor의 조작 예시   

##0. PyTorch 설치 및 불러오기   


```python
!pip install torch==2.0.1
```

    Collecting torch==2.0.1
      Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m619.9/619.9 MB[0m [31m2.1 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12.1)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)
    Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.0/21.0 MB[0m [31m44.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m849.3/849.3 kB[0m [31m49.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)
      Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.8/11.8 MB[0m [31m57.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)
      Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m557.1/557.1 MB[0m [31m2.0 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)
      Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m317.1/317.1 MB[0m [31m4.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)
      Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m168.4/168.4 MB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)
      Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m54.6/54.6 MB[0m [31m9.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)
      Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m102.6/102.6 MB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)
      Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m173.2/173.2 MB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)
      Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m177.1/177.1 MB[0m [31m7.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)
      Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m98.6/98.6 kB[0m [31m13.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting triton==2.0.0 (from torch==2.0.1)
      Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m63.3/63.3 MB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)
    Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)
    Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)
    Collecting lit (from triton==2.0.0->torch==2.0.1)
      Downloading lit-18.1.7-py3-none-any.whl (96 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m96.4/96.4 kB[0m [31m11.7 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)
    Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)
    Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch
      Attempting uninstall: triton
        Found existing installation: triton 2.3.0
        Uninstalling triton-2.3.0:
          Successfully uninstalled triton-2.3.0
      Attempting uninstall: torch
        Found existing installation: torch 2.3.0+cu121
        Uninstalling torch-2.3.0+cu121:
          Successfully uninstalled torch-2.3.0+cu121
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.[0m[31m
    [0mSuccessfully installed lit-18.1.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0
    

##1. 텐서 연산 및 조작   


###1.1 텐서 간의 계산 실습   

1.1.1 텐서 간의 사칙연산


```python
import torch

tensor_a = torch.tensor([[1,-1],[2,3]])
tensor_b = torch.tensor([[2,-2],[3,1]])

print('덧셈')
print('a + b: \n', tensor_a + tensor_b)
print('\n')
print('torch.add(a,b): \n', torch.add(tensor_a, tensor_b))

print('--------' * 5)

print('뺄셈')
print('a - b: \n', tensor_a - tensor_b)
print('\n')
print('torch.sub(a,b): \n', torch.sub(tensor_a, tensor_b))

print('--------' * 5)

print('곱셈')
print('a * b: \n', tensor_a * tensor_b)
print('\n')
print('torch.mul(a,b): \n', torch.mul(tensor_a, tensor_b))

print('--------' * 5)

print('나눗셈')
print('a / b: \n', tensor_a / tensor_b)
print('\n')
print('torch.div(a,b): \n', torch.div(tensor_a, tensor_b))
```

    덧셈
    a + b: 
     tensor([[ 3, -3],
            [ 5,  4]])
    
    
    torch.add(a,b): 
     tensor([[ 3, -3],
            [ 5,  4]])
    ----------------------------------------
    뺄셈
    a - b: 
     tensor([[-1,  1],
            [-1,  2]])
    
    
    torch.sub(a,b): 
     tensor([[-1,  1],
            [-1,  2]])
    ----------------------------------------
    곱셈
    a * b: 
     tensor([[2, 2],
            [6, 3]])
    
    
    torch.mul(a,b): 
     tensor([[2, 2],
            [6, 3]])
    ----------------------------------------
    나눗셈
    a / b: 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    
    
    torch.div(a,b): 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    

1.1.2 텐서의 통계치


```python
# sum: 텐서의 원소들의 합을 반환
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension 지정 안 했을 때: ', torch.sum(tensor_a))
print('dim=0일 때: ', torch.sum(tensor_a, dim=0))
print('dim=1일 때: ', torch.sum(tensor_a, dim=1))
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension 지정 안 했을 때:  tensor(10)
    dim=0일 때:  tensor([4, 6])
    dim=1일 때:  tensor([3, 7])
    


```python
# mean: 텐서의 원소들의 평균을 반환
tensor_a = torch.tensor([[1,2],[3,4]], dtype=torch.float32) # mean은 실수가 나올 수 있으므로 float로 지정해줘야 함
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension 지정 안 했을 때: ', torch.mean(tensor_a))
print('dim=0일 때: ', torch.mean(tensor_a, dim=0))
print('dim=1일 때: ', torch.mean(tensor_a, dim=1))
```

    tensor([[1., 2.],
            [3., 4.]])
    Shape:  torch.Size([2, 2])
    
    
    dimension 지정 안 했을 때:  tensor(2.5000)
    dim=0일 때:  tensor([2., 3.])
    dim=1일 때:  tensor([1.5000, 3.5000])
    


```python
# max, min: 텐서의 원소들의 가장 큰 값과 작은 값을 반환
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension 지정 안 했을 때: ', torch.max(tensor_a))
print('dim=0일 때: ', torch.max(tensor_a, dim=0).values)
print('dim=1일 때: ', torch.max(tensor_a, dim=1).values)
print('\n')

print('dimension 지정 안 했을 때: ', torch.min(tensor_a))
print('dim=0일 때: ', torch.min(tensor_a, dim=0).values)
print('dim=1일 때: ', torch.min(tensor_a, dim=1).values)
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension 지정 안 했을 때:  tensor(4)
    dim=0일 때:  tensor([3, 4])
    dim=1일 때:  tensor([2, 4])
    
    
    dimension 지정 안 했을 때:  tensor(1)
    dim=0일 때:  tensor([1, 2])
    dim=1일 때:  tensor([1, 3])
    


```python
# argmax, argmin: 텐서의 원소들의 가장 큰 값과 작은 값의 위치 변환
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension 지정 안 했을 때: ', torch.argmax(tensor_a))
print('dim=0일 때: ', torch.argmax(tensor_a, dim=0))
print('dim=1일 때: ', torch.argmax(tensor_a, dim=1))
print('\n')

print('dimension 지정 안 했을 때: ', torch.argmin(tensor_a))
print('dim=0일 때: ', torch.argmin(tensor_a, dim=0))
print('dim=1일 때: ', torch.argmin(tensor_a, dim=1))
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension 지정 안 했을 때:  tensor(3)
    dim=0일 때:  tensor([1, 1])
    dim=1일 때:  tensor([1, 1])
    
    
    dimension 지정 안 했을 때:  tensor(0)
    dim=0일 때:  tensor([0, 0])
    dim=1일 때:  tensor([0, 0])
    


```python
# dot: 백터의 내적(inner product) 반환
# torch.dot(a,b) 또는 a.dot(b)로 쓸 수 있음
v1 = torch.tensor([1,2])
u1 = torch.tensor([3,4])

print('v1.dot(u1): ', v1.dot(u1))
print('torch.dot(v1,u1): ', torch.dot(v1,u1))
```

    v1.dot(u1):  tensor(11)
    torch.dot(v1,u1):  tensor(11)
    


```python
# matmul: 두 텐서 간의 행렬곱 반환 (원소곱과 다름 주의!)
# torch.matmul(a,b) 또는 a.matmul(b)로 쓸 수 있음
A = torch.tensor([[1,2],[3,4]])
B = torch.tensor([[-1,2],[1,0]])
print('A: ', A)
print('B: ', B)
print('\n')

print('AB: ', torch.matmul(A,B))
print('BA: ', A.matmul(B))
```

    A:  tensor([[1, 2],
            [3, 4]])
    B:  tensor([[-1,  2],
            [ 1,  0]])
    
    
    AB:  tensor([[1, 2],
            [1, 6]])
    BA:  tensor([[5, 6],
            [1, 2]])
    

###1.2 Broadcasting을 이용한 텐서값 변경   


```python
# scalar 값으로 텐서 원소 변경하기
tensor_a = torch.randn(3,2)
print('Original: \n', tensor_a)
print('\n')

tensor_a[0,:] = 10 # 0행의 모든 열에 broadcasting을 통한 scalar값 대입
print('변경된 텐서: \n', tensor_a)
```

    Original: 
     tensor([[-0.2202,  0.6797],
            [-0.0798,  0.1272],
            [ 0.6606,  1.2014]])
    
    
    변경된 텐서: 
     tensor([[10.0000, 10.0000],
            [-0.0798,  0.1272],
            [ 0.6606,  1.2014]])
    


```python
# 텐서 값으로 텐서 원소 변경하기
# scalar 값으로 텐서 원소 변경하기
tensor_a = torch.randn(3,2)
print('Original: \n', tensor_a)
print('\n')

tensor_a[:,:] = torch.tensor([0,1]) # 모든 값에 접근하여 [0,1]로 변경
print('변경된 텐서: \n', tensor_a)
```

    Original: 
     tensor([[ 0.0293, -0.4313],
            [-0.4239, -0.2054],
            [-1.7345, -1.3470]])
    
    
    변경된 텐서: 
     tensor([[0., 1.],
            [0., 1.],
            [0., 1.]])
    

###1.3 Broadcasting을 이용한 차원이 다른 텐서 간의 계산 실습


```python
# 차원이 서로 다른 텐서 간의 계산
tensor_a = torch.eye(3)
print('Tensor A: \n', tensor_a)
print('\n')

tensor_b = torch.tensor([1,2,3])
print('Tensor B: \n', tensor_b)
print('\n')

print('A + B: \n', tensor_a + tensor_b) # broadcasting을 통해 (3,)인 B가 (3,3)으로 변환되어 계산 (행의 확장)
```

    Tensor A: 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B: 
     tensor([1, 2, 3])
    
    
    A + B: 
     tensor([[2., 2., 3.],
            [1., 3., 3.],
            [1., 2., 4.]])
    


```python
# 차원이 서로 다른 텐서 간의 계산
tensor_a = torch.eye(3)
print('Tensor A: \n', tensor_a)
print('\n')

tensor_b = torch.tensor([1,2,3]).reshape(3,1)
print('Tensor B: \n', tensor_b)
print('\n')

print('A + B: \n', tensor_a + tensor_b)
```

    Tensor A: 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B: 
     tensor([[1],
            [2],
            [3]])
    
    
    A + B: 
     tensor([[2., 1., 1.],
            [2., 3., 2.],
            [3., 3., 4.]])
    


```python
# 차원이 맞지 않는 경우 차원을 추가해서 broadcasting으로 텐서 간의 계산을 할 수 있음
tensor_a = torch.randn(3,2,5)
mean_a = tensor_a.mean(2) # dim=2(열) 기준 평균값
print(f'Tensor size: {tensor_a.size()}, mean size: {mean_a.size()}')
print('\n')

print(tensor_a - mean_a) # 에러발생. 차원이 달라서 계산이 되지 않음
```

    Tensor size: torch.Size([3, 2, 5]), mean size: torch.Size([3, 2])
    
    
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-17-90398956301f> in <cell line: 7>()
          5 print('\n')
          6 
    ----> 7 print(tensor_a - mean_a)
    

    RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2



```python
# 차원 생성 후 broadcasting 하면 됨
unsq_mean = mean_a.unsqueeze(-1)  # 마지막 축 추가
print(unsq_mean.size())
print('\n')

print(tensor_a - unsq_mean) # 에러 안 뜨고 잘 계산 됨!
```

    torch.Size([3, 2, 1])
    
    
    tensor([[[-0.5421,  0.4017, -0.3736,  0.9464, -0.4324],
             [ 1.2773, -1.5767, -1.4332,  1.6696,  0.0630]],
    
            [[-0.4313,  1.0731, -1.3158,  0.7565, -0.0825],
             [-0.8651, -0.0064, -0.0283,  1.1919, -0.2920]],
    
            [[-0.1381,  0.0026,  0.1413, -0.6306,  0.6248],
             [-0.3796, -1.1089, -0.3091,  0.8830,  0.9147]]])
    

##2. Sparse Tensor 조작 및 실습   

###2.1 COO Tensor에 대한 이해 및 실습   


```python
a = torch.tensor([[0,2],[3,0]])
a.to_sparse() # COO Sparse tensor로 변환
```




    tensor(indices=tensor([[0, 1],
                           [1, 0]]),
           values=tensor([2, 3]),
           size=(2, 2), nnz=2, layout=torch.sparse_coo)



위 예시는 dense tensor를 만들 수 있을 때만 사용 가능한데 그럼 그렇지 않은 경우에는 어떻게 COO 텐서를 만들 수 있을까?   
=> torch.sparce_coo_tensor를 사용해서 dense tensor를 만들지 않고 처음부터 sparse tensor를 생성하는 방법이 있음


```python
# sparse_coo_tensor: COO 형식의 sparse tensor를 생성하는 함수
# indices: 0이 아닌 값을 가진 행,열의 위치 / values: 0이 아닌 값 / nnz: 0이 아닌 값의 개수
indices = torch.tensor([[0,1,1],[2,0,1]])
values = torch.tensor([4,5,6])
sparse_tensor = torch.sparse_coo_tensor(indices = indices, values = values, size=(2,3)) # (2,3)의 sparse tensor

print(sparse_tensor)
print('\n')
print(sparse_tensor.to_dense())
```

    tensor(indices=tensor([[0, 1, 1],
                           [2, 0, 1]]),
           values=tensor([4, 5, 6]),
           size=(2, 3), nnz=3, layout=torch.sparse_coo)
    
    
    tensor([[0, 0, 4],
            [5, 6, 0]])
    

###2.2 CSC/CSR Tensor에 대한 이해 및 실습   

- Compressed Sparse Column, Compressed Sparse Row

2.2.1 CSR Sparse Tensor(Compressed Sparse Row)로 변환


```python
# to_sparse_csr: Dense tensor를 CSR 형식의 Sparse tensor로 변환하는 함수
# crow_indices: 0이 아닌 값을 가진 행의 위치(첫번째는 무조건 0) / col_indices: 0이 아닌 값을 가진 열의 위치 / values: 0이 아닌 값 / nnz: 0이 아닌 값의 개수
t = torch.tensor([[0,0,4,3],[5,6,0,0]])
print('Share: ', t.size())
print(t)
print('\n')

t.to_sparse_csr() # Dense Tensor를 CSR Sparse Tensor 형식으로 변환
```

    Share:  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(crow_indices=tensor([0, 2, 4]),
           col_indices=tensor([2, 3, 0, 1]),
           values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,
           layout=torch.sparse_csr)



2.2.2 CSC Sparse Tensor(Compressed Sparse Column)로 변환


```python
# to_sparse_csc: Dense tensor를 CSC 형식의 Sparse tensor로 변환하는 함수
# crow_indices: 0이 아닌 값을 가진 열의 위치(첫번째는 무조건 0) / col_indices: 0이 아닌 값을 가진 행 위치 / values: 0이 아닌 값 / nnz: 0이 아닌 값의 개수
t = torch.tensor([[0,0,4,3],[5,6,0,0]])
print('Share: ', t.size())
print(t)
print('\n')

t.to_sparse_csc() # Dense Tensor를 CSR Sparse Tensor 형식으로 변환
```

    Share:  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),
           row_indices=tensor([1, 1, 0, 0]),
           values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,
           layout=torch.sparse_csc)



2.2.3 CSR Soarse Tensor 생성


```python
# sparce_csr_tensor: CSR 형식의 Sparse tensor를 생성하는 함수
crow_indices = torch.tensor([0,2,2])
col_indices = torch.tensor([0,1])
values = torch.tensor([1,2])
csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)

print(csr)
print('\n')
print(csr.to_dense())
```

    tensor(crow_indices=tensor([0, 2, 2]),
           col_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)
    
    
    tensor([[1, 2],
            [0, 0]])
    


```python
# sparce_csc_tensor: CSC 형식의 Sparse tensor를 생성하는 함수
ccol_indices = torch.tensor([0,2,2])
row_indices = torch.tensor([0,1])
values = torch.tensor([1,2])
csc = torch.sparse_csc_tensor(ccol_indices = ccol_indices, row_indices = row_indices, values = values)

print(csr)
print('\n')
print(csc.to_dense())
```

    tensor(ccol_indices=tensor([0, 2, 2]),
           row_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csc)
    
    
    tensor([[1, 0],
            [2, 0]])
    

###2.3 Sparse Tensor의 필요성 이해 및 실습   

- 아주 큰 크기의 matrix를 구성할 때, 일반적인 dense tensor는 메뫼 아웃 현상이 발생하지만, sparse tensor는 메모리 아웃 현상이 발생하지 않음


```python
# to_dense(): sparse tensor를 dense tensor로 만드는 함수
i = torch.randint(0,100000,(200000,)).reshape(2,-1)
v = torch.rand(100000)
coo_sparse_tensor = torch.sparse_coo_tensor(indices = i, values = v, size = (100000,100000)) # COO Sparse Tensor (100000x100000)
coo_sparse_tensor
```




    tensor(indices=tensor([[ 8079, 96765, 24033,  ..., 74971, 86518, 39290],
                           [20287, 71308, 80797,  ..., 24146, 21949,  2927]]),
           values=tensor([0.0174, 0.4642, 0.3821,  ..., 0.4562, 0.5861, 0.9411]),
           size=(100000, 100000), nnz=100000, layout=torch.sparse_coo)




```python
crow = torch.randint(0,100000,(100000,))
col = torch.randint(0,100000,(100000,))
v = torch.rand(100000)
csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000x100000)
csr_sparse_tensor
```




    tensor(crow_indices=tensor([33096, 94883, 74398,  ...,  8843,  7862, 42789]),
           col_indices=tensor([ 9868, 84780,  2244,  ...,  3275, 76017, 21069]),
           values=tensor([0.0563, 0.0192, 0.9881,  ..., 0.1575, 0.0997, 0.6914]),
           size=(99999, 100000), nnz=100000, layout=torch.sparse_csr)




```python
# COO 형식으로 만들어진 Sparse Tensor를 Dense Tensor로 변환, 메모리 아웃
coo_sparse_tensor.to_dense() # 세션 다운됨
```


```python
# 커널을 재시작 하기에 다시 torch library 로드
import torch
```


```python
# CSR 형식으로 만들어진 Sparse Tensor를 Dense Tensor로 변환, 메모리 아웃
csr_sparse_tensor.to_dense() # 세션 다운됨
```


```python
# 커널을 재시작 하기에 다시 torch library 로드
import torch
```

###2.4 Sparse Tensor의 조작 예시   


```python
# Sparse와 Sparse Tensor 간의 연산 (2차원)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float)
b = torch.tensor([[1,0],[0,0]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('덧셈')
print(torch.add(a,b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())
print('\n')

print('곱셈')
print(torch.mul(a,b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())
print('\n')

print('행렬곱')
print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())
```

    덧셈
    tensor([[True, True],
            [True, True]])
    
    
    곱셈
    tensor([[True, True],
            [True, True]])
    
    
    행렬곱
    tensor([[True, True],
            [True, True]])
    


```python
# Sparse와 Sparse Tensor 간의 연산 (3차원)
a = torch.tensor([[[0,1],[0,2]],[[0,1],[0,2]]], dtype=torch.float)
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('덧셈')
print(torch.add(a,b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())
print('\n')

print('곱셈')
print(torch.mul(a,b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())
print('\n')

print('행렬곱')
print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # 에러 발생. 3차원끼리는 행렬곱이 불가능함
```

    덧셈
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    곱셈
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    행렬곱
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-4-55a52bea0e54> in <cell line: 17>()
         15 
         16 print('행렬곱')
    ---> 17 print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    



```python
# Dense와 Sparse Tensor 간의 연산(2차원)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float)
b = torch.tensor([[1,0],[0,0]], dtype=torch.float)

sparse_b = b.to_sparse()

print('덧셈')
print(torch.add(a,b).to_dense() == torch.add(a, sparse_b).to_dense())
print('\n')

print('곱셈')
print(torch.mul(a,b).to_dense() == torch.mul(a, sparse_b).to_dense())
print('\n')

print('행렬곱')
print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense())
```

    덧셈
    tensor([[True, True],
            [True, True]])
    
    
    곱셈
    tensor([[True, True],
            [True, True]])
    
    
    행렬곱
    tensor([[True, True],
            [True, True]])
    


```python
# Dense와 Sparse Tensor 간의 연산(3차원)
a = torch.tensor([[[0,1],[0,2]],[[0,1],[0,2]]], dtype=torch.float)
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_b = b.to_sparse()

print('덧셈')
print(torch.add(a,b).to_dense() == torch.add(a, sparse_b).to_dense())
print('\n')

print('곱셈')
print(torch.mul(a,b).to_dense() == torch.mul(a, sparse_b).to_dense())
print('\n')

print('행렬곱')
print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense()) # 에러 발생
```

    덧셈
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    곱셈
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    행렬곱
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-7-26f355b228ca> in <cell line: 16>()
         14 
         15 print('행렬곱')
    ---> 16 print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense())
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    



```python
# Sparse Tensor의 Indexing
a = torch.tensor([[0,1],[0,2]])
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('2차원 Sparse Tensor 인덱싱')
print(a[0] == sparse_a[0].to_dense())
print('\n')

print('3차원 Sparse Tensor 인덱싱')
print(b[0] == sparse_b[0].to_dense())
```

    2차원 Sparse Tensor 인덱싱
    tensor([True, True])
    
    
    3차원 Sparse Tensor 인덱싱
    tensor([[True, True],
            [True, True]])
    


```python
# 2차원 Sparse Tensor(CSR)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float).to_sparse_csr()
print(a[0].to_dense)
```

    <built-in method to_dense of Tensor object at 0x7abda8737ec0>
    


```python
a[0,:] # 파이토치 내부에서 sparce tensor에 대한 슬라이싱 구현이 안 되어있기 때문
```


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-8-d9d9ecd2ddf9> in <cell line: 1>()
    ----> 1 a[0,:] # 파이토치 내부에서 sparce tensor에 대한 슬라이싱 구현이 안 되어있기 때문
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22129 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4930 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16910 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]
    BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]
    PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]
    

