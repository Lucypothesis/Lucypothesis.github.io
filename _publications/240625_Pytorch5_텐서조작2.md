---
layout: post
title: "240625(í™”) [ì˜¨ë¼ì¸ê°•ì˜] Pytorch4_í…ì„œì¡°ì‘(2)"
subtitle: "[Tips]"
date: 2024-06-25 22:21
background: 
tag: [Tips, Github io, Notion]
---

# [ì˜¨ë¼ì¸ê°•ì˜]Pytorch5_í…ì„œì¡°ì‘(2)

### **~ ëª©ì°¨ ~**
0. PyTorch ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°   
1. í…ì„œ ì—°ì‚° ë° ì¡°ì‘   
  1.1 í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ   
  1.2 Broadcastingì„ ì´ìš©í•œ í…ì„œê°’ ë³€ê²½   
  1.3 Broadcastingì„ ì´ìš©í•œ ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ   
2. Sparse Tensor ì¡°ì‘ ë° ì‹¤ìŠµ   
  2.1 COO Tensorì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ   
  2.2 CSC/CSR Tensorì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ   
  2.3 Sparse Tensorì˜ í•„ìš”ì„± ì´í•´ ë° ì‹¤ìŠµ   
  2.4 Sparse Tensorì˜ ì¡°ì‘ ì˜ˆì‹œ   

##0. PyTorch ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°   


```python
!pip install torch==2.0.1
```

    Collecting torch==2.0.1
      Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m619.9/619.9 MB[0m [31m2.1 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12.1)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)
    Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.0/21.0 MB[0m [31m44.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m849.3/849.3 kB[0m [31m49.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)
      Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.8/11.8 MB[0m [31m57.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)
      Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m557.1/557.1 MB[0m [31m2.0 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)
      Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m317.1/317.1 MB[0m [31m4.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)
      Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m168.4/168.4 MB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)
      Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m54.6/54.6 MB[0m [31m9.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)
      Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m102.6/102.6 MB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)
      Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m173.2/173.2 MB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)
      Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m177.1/177.1 MB[0m [31m7.4 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)
      Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m98.6/98.6 kB[0m [31m13.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting triton==2.0.0 (from torch==2.0.1)
      Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m63.3/63.3 MB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)
    Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)
    Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)
    Collecting lit (from triton==2.0.0->torch==2.0.1)
      Downloading lit-18.1.7-py3-none-any.whl (96 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m96.4/96.4 kB[0m [31m11.7 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)
    Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)
    Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch
      Attempting uninstall: triton
        Found existing installation: triton 2.3.0
        Uninstalling triton-2.3.0:
          Successfully uninstalled triton-2.3.0
      Attempting uninstall: torch
        Found existing installation: torch 2.3.0+cu121
        Uninstalling torch-2.3.0+cu121:
          Successfully uninstalled torch-2.3.0+cu121
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.[0m[31m
    [0mSuccessfully installed lit-18.1.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0
    

##1. í…ì„œ ì—°ì‚° ë° ì¡°ì‘   


###1.1 í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ   

1.1.1 í…ì„œ ê°„ì˜ ì‚¬ì¹™ì—°ì‚°


```python
import torch

tensor_a = torch.tensor([[1,-1],[2,3]])
tensor_b = torch.tensor([[2,-2],[3,1]])

print('ë§ì…ˆ')
print('a + b: \n', tensor_a + tensor_b)
print('\n')
print('torch.add(a,b): \n', torch.add(tensor_a, tensor_b))

print('--------' * 5)

print('ëº„ì…ˆ')
print('a - b: \n', tensor_a - tensor_b)
print('\n')
print('torch.sub(a,b): \n', torch.sub(tensor_a, tensor_b))

print('--------' * 5)

print('ê³±ì…ˆ')
print('a * b: \n', tensor_a * tensor_b)
print('\n')
print('torch.mul(a,b): \n', torch.mul(tensor_a, tensor_b))

print('--------' * 5)

print('ë‚˜ëˆ—ì…ˆ')
print('a / b: \n', tensor_a / tensor_b)
print('\n')
print('torch.div(a,b): \n', torch.div(tensor_a, tensor_b))
```

    ë§ì…ˆ
    a + b: 
     tensor([[ 3, -3],
            [ 5,  4]])
    
    
    torch.add(a,b): 
     tensor([[ 3, -3],
            [ 5,  4]])
    ----------------------------------------
    ëº„ì…ˆ
    a - b: 
     tensor([[-1,  1],
            [-1,  2]])
    
    
    torch.sub(a,b): 
     tensor([[-1,  1],
            [-1,  2]])
    ----------------------------------------
    ê³±ì…ˆ
    a * b: 
     tensor([[2, 2],
            [6, 3]])
    
    
    torch.mul(a,b): 
     tensor([[2, 2],
            [6, 3]])
    ----------------------------------------
    ë‚˜ëˆ—ì…ˆ
    a / b: 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    
    
    torch.div(a,b): 
     tensor([[0.5000, 0.5000],
            [0.6667, 3.0000]])
    

1.1.2 í…ì„œì˜ í†µê³„ì¹˜


```python
# sum: í…ì„œì˜ ì›ì†Œë“¤ì˜ í•©ì„ ë°˜í™˜
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.sum(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.sum(tensor_a, dim=0))
print('dim=1ì¼ ë•Œ: ', torch.sum(tensor_a, dim=1))
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(10)
    dim=0ì¼ ë•Œ:  tensor([4, 6])
    dim=1ì¼ ë•Œ:  tensor([3, 7])
    


```python
# mean: í…ì„œì˜ ì›ì†Œë“¤ì˜ í‰ê· ì„ ë°˜í™˜
tensor_a = torch.tensor([[1,2],[3,4]], dtype=torch.float32) # meanì€ ì‹¤ìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ floatë¡œ ì§€ì •í•´ì¤˜ì•¼ í•¨
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.mean(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.mean(tensor_a, dim=0))
print('dim=1ì¼ ë•Œ: ', torch.mean(tensor_a, dim=1))
```

    tensor([[1., 2.],
            [3., 4.]])
    Shape:  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(2.5000)
    dim=0ì¼ ë•Œ:  tensor([2., 3.])
    dim=1ì¼ ë•Œ:  tensor([1.5000, 3.5000])
    


```python
# max, min: í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ í° ê°’ê³¼ ì‘ì€ ê°’ì„ ë°˜í™˜
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.max(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.max(tensor_a, dim=0).values)
print('dim=1ì¼ ë•Œ: ', torch.max(tensor_a, dim=1).values)
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.min(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.min(tensor_a, dim=0).values)
print('dim=1ì¼ ë•Œ: ', torch.min(tensor_a, dim=1).values)
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(4)
    dim=0ì¼ ë•Œ:  tensor([3, 4])
    dim=1ì¼ ë•Œ:  tensor([2, 4])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(1)
    dim=0ì¼ ë•Œ:  tensor([1, 2])
    dim=1ì¼ ë•Œ:  tensor([1, 3])
    


```python
# argmax, argmin: í…ì„œì˜ ì›ì†Œë“¤ì˜ ê°€ì¥ í° ê°’ê³¼ ì‘ì€ ê°’ì˜ ìœ„ì¹˜ ë³€í™˜
tensor_a = torch.tensor([[1,2],[3,4]])
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.argmax(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.argmax(tensor_a, dim=0))
print('dim=1ì¼ ë•Œ: ', torch.argmax(tensor_a, dim=1))
print('\n')

print('dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ: ', torch.argmin(tensor_a))
print('dim=0ì¼ ë•Œ: ', torch.argmin(tensor_a, dim=0))
print('dim=1ì¼ ë•Œ: ', torch.argmin(tensor_a, dim=1))
```

    tensor([[1, 2],
            [3, 4]])
    Shape:  torch.Size([2, 2])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(3)
    dim=0ì¼ ë•Œ:  tensor([1, 1])
    dim=1ì¼ ë•Œ:  tensor([1, 1])
    
    
    dimension ì§€ì • ì•ˆ í–ˆì„ ë•Œ:  tensor(0)
    dim=0ì¼ ë•Œ:  tensor([0, 0])
    dim=1ì¼ ë•Œ:  tensor([0, 0])
    


```python
# dot: ë°±í„°ì˜ ë‚´ì (inner product) ë°˜í™˜
# torch.dot(a,b) ë˜ëŠ” a.dot(b)ë¡œ ì“¸ ìˆ˜ ìˆìŒ
v1 = torch.tensor([1,2])
u1 = torch.tensor([3,4])

print('v1.dot(u1): ', v1.dot(u1))
print('torch.dot(v1,u1): ', torch.dot(v1,u1))
```

    v1.dot(u1):  tensor(11)
    torch.dot(v1,u1):  tensor(11)
    


```python
# matmul: ë‘ í…ì„œ ê°„ì˜ í–‰ë ¬ê³± ë°˜í™˜ (ì›ì†Œê³±ê³¼ ë‹¤ë¦„ ì£¼ì˜!)
# torch.matmul(a,b) ë˜ëŠ” a.matmul(b)ë¡œ ì“¸ ìˆ˜ ìˆìŒ
A = torch.tensor([[1,2],[3,4]])
B = torch.tensor([[-1,2],[1,0]])
print('A: ', A)
print('B: ', B)
print('\n')

print('AB: ', torch.matmul(A,B))
print('BA: ', A.matmul(B))
```

    A:  tensor([[1, 2],
            [3, 4]])
    B:  tensor([[-1,  2],
            [ 1,  0]])
    
    
    AB:  tensor([[1, 2],
            [1, 6]])
    BA:  tensor([[5, 6],
            [1, 2]])
    

###1.2 Broadcastingì„ ì´ìš©í•œ í…ì„œê°’ ë³€ê²½   


```python
# scalar ê°’ìœ¼ë¡œ í…ì„œ ì›ì†Œ ë³€ê²½í•˜ê¸°
tensor_a = torch.randn(3,2)
print('Original: \n', tensor_a)
print('\n')

tensor_a[0,:] = 10 # 0í–‰ì˜ ëª¨ë“  ì—´ì— broadcastingì„ í†µí•œ scalarê°’ ëŒ€ì…
print('ë³€ê²½ëœ í…ì„œ: \n', tensor_a)
```

    Original: 
     tensor([[-0.2202,  0.6797],
            [-0.0798,  0.1272],
            [ 0.6606,  1.2014]])
    
    
    ë³€ê²½ëœ í…ì„œ: 
     tensor([[10.0000, 10.0000],
            [-0.0798,  0.1272],
            [ 0.6606,  1.2014]])
    


```python
# í…ì„œ ê°’ìœ¼ë¡œ í…ì„œ ì›ì†Œ ë³€ê²½í•˜ê¸°
# scalar ê°’ìœ¼ë¡œ í…ì„œ ì›ì†Œ ë³€ê²½í•˜ê¸°
tensor_a = torch.randn(3,2)
print('Original: \n', tensor_a)
print('\n')

tensor_a[:,:] = torch.tensor([0,1]) # ëª¨ë“  ê°’ì— ì ‘ê·¼í•˜ì—¬ [0,1]ë¡œ ë³€ê²½
print('ë³€ê²½ëœ í…ì„œ: \n', tensor_a)
```

    Original: 
     tensor([[ 0.0293, -0.4313],
            [-0.4239, -0.2054],
            [-1.7345, -1.3470]])
    
    
    ë³€ê²½ëœ í…ì„œ: 
     tensor([[0., 1.],
            [0., 1.],
            [0., 1.]])
    

###1.3 Broadcastingì„ ì´ìš©í•œ ì°¨ì›ì´ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚° ì‹¤ìŠµ


```python
# ì°¨ì›ì´ ì„œë¡œ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚°
tensor_a = torch.eye(3)
print('Tensor A: \n', tensor_a)
print('\n')

tensor_b = torch.tensor([1,2,3])
print('Tensor B: \n', tensor_b)
print('\n')

print('A + B: \n', tensor_a + tensor_b) # broadcastingì„ í†µí•´ (3,)ì¸ Bê°€ (3,3)ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ê³„ì‚° (í–‰ì˜ í™•ì¥)
```

    Tensor A: 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B: 
     tensor([1, 2, 3])
    
    
    A + B: 
     tensor([[2., 2., 3.],
            [1., 3., 3.],
            [1., 2., 4.]])
    


```python
# ì°¨ì›ì´ ì„œë¡œ ë‹¤ë¥¸ í…ì„œ ê°„ì˜ ê³„ì‚°
tensor_a = torch.eye(3)
print('Tensor A: \n', tensor_a)
print('\n')

tensor_b = torch.tensor([1,2,3]).reshape(3,1)
print('Tensor B: \n', tensor_b)
print('\n')

print('A + B: \n', tensor_a + tensor_b)
```

    Tensor A: 
     tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])
    
    
    Tensor B: 
     tensor([[1],
            [2],
            [3]])
    
    
    A + B: 
     tensor([[2., 1., 1.],
            [2., 3., 2.],
            [3., 3., 4.]])
    


```python
# ì°¨ì›ì´ ë§ì§€ ì•ŠëŠ” ê²½ìš° ì°¨ì›ì„ ì¶”ê°€í•´ì„œ broadcastingìœ¼ë¡œ í…ì„œ ê°„ì˜ ê³„ì‚°ì„ í•  ìˆ˜ ìˆìŒ
tensor_a = torch.randn(3,2,5)
mean_a = tensor_a.mean(2) # dim=2(ì—´) ê¸°ì¤€ í‰ê· ê°’
print(f'Tensor size: {tensor_a.size()}, mean size: {mean_a.size()}')
print('\n')

print(tensor_a - mean_a) # ì—ëŸ¬ë°œìƒ. ì°¨ì›ì´ ë‹¬ë¼ì„œ ê³„ì‚°ì´ ë˜ì§€ ì•ŠìŒ
```

    Tensor size: torch.Size([3, 2, 5]), mean size: torch.Size([3, 2])
    
    
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-17-90398956301f> in <cell line: 7>()
          5 print('\n')
          6 
    ----> 7 print(tensor_a - mean_a)
    

    RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2



```python
# ì°¨ì› ìƒì„± í›„ broadcasting í•˜ë©´ ë¨
unsq_mean = mean_a.unsqueeze(-1)  # ë§ˆì§€ë§‰ ì¶• ì¶”ê°€
print(unsq_mean.size())
print('\n')

print(tensor_a - unsq_mean) # ì—ëŸ¬ ì•ˆ ëœ¨ê³  ì˜ ê³„ì‚° ë¨!
```

    torch.Size([3, 2, 1])
    
    
    tensor([[[-0.5421,  0.4017, -0.3736,  0.9464, -0.4324],
             [ 1.2773, -1.5767, -1.4332,  1.6696,  0.0630]],
    
            [[-0.4313,  1.0731, -1.3158,  0.7565, -0.0825],
             [-0.8651, -0.0064, -0.0283,  1.1919, -0.2920]],
    
            [[-0.1381,  0.0026,  0.1413, -0.6306,  0.6248],
             [-0.3796, -1.1089, -0.3091,  0.8830,  0.9147]]])
    

##2. Sparse Tensor ì¡°ì‘ ë° ì‹¤ìŠµ   

###2.1 COO Tensorì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ   


```python
a = torch.tensor([[0,2],[3,0]])
a.to_sparse() # COO Sparse tensorë¡œ ë³€í™˜
```




    tensor(indices=tensor([[0, 1],
                           [1, 0]]),
           values=tensor([2, 3]),
           size=(2, 2), nnz=2, layout=torch.sparse_coo)



ìœ„ ì˜ˆì‹œëŠ” dense tensorë¥¼ ë§Œë“¤ ìˆ˜ ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•œë° ê·¸ëŸ¼ ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ COO í…ì„œë¥¼ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œ?   
=> torch.sparce_coo_tensorë¥¼ ì‚¬ìš©í•´ì„œ dense tensorë¥¼ ë§Œë“¤ì§€ ì•Šê³  ì²˜ìŒë¶€í„° sparse tensorë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ìˆìŒ


```python
# sparse_coo_tensor: COO í˜•ì‹ì˜ sparse tensorë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
# indices: 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ í–‰,ì—´ì˜ ìœ„ì¹˜ / values: 0ì´ ì•„ë‹Œ ê°’ / nnz: 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
indices = torch.tensor([[0,1,1],[2,0,1]])
values = torch.tensor([4,5,6])
sparse_tensor = torch.sparse_coo_tensor(indices = indices, values = values, size=(2,3)) # (2,3)ì˜ sparse tensor

print(sparse_tensor)
print('\n')
print(sparse_tensor.to_dense())
```

    tensor(indices=tensor([[0, 1, 1],
                           [2, 0, 1]]),
           values=tensor([4, 5, 6]),
           size=(2, 3), nnz=3, layout=torch.sparse_coo)
    
    
    tensor([[0, 0, 4],
            [5, 6, 0]])
    

###2.2 CSC/CSR Tensorì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ   

- Compressed Sparse Column, Compressed Sparse Row

2.2.1 CSR Sparse Tensor(Compressed Sparse Row)ë¡œ ë³€í™˜


```python
# to_sparse_csr: Dense tensorë¥¼ CSR í˜•ì‹ì˜ Sparse tensorë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
# crow_indices: 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ í–‰ì˜ ìœ„ì¹˜(ì²«ë²ˆì§¸ëŠ” ë¬´ì¡°ê±´ 0) / col_indices: 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ ì—´ì˜ ìœ„ì¹˜ / values: 0ì´ ì•„ë‹Œ ê°’ / nnz: 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
t = torch.tensor([[0,0,4,3],[5,6,0,0]])
print('Share: ', t.size())
print(t)
print('\n')

t.to_sparse_csr() # Dense Tensorë¥¼ CSR Sparse Tensor í˜•ì‹ìœ¼ë¡œ ë³€í™˜
```

    Share:  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(crow_indices=tensor([0, 2, 4]),
           col_indices=tensor([2, 3, 0, 1]),
           values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,
           layout=torch.sparse_csr)



2.2.2 CSC Sparse Tensor(Compressed Sparse Column)ë¡œ ë³€í™˜


```python
# to_sparse_csc: Dense tensorë¥¼ CSC í˜•ì‹ì˜ Sparse tensorë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
# crow_indices: 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ ì—´ì˜ ìœ„ì¹˜(ì²«ë²ˆì§¸ëŠ” ë¬´ì¡°ê±´ 0) / col_indices: 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§„ í–‰ ìœ„ì¹˜ / values: 0ì´ ì•„ë‹Œ ê°’ / nnz: 0ì´ ì•„ë‹Œ ê°’ì˜ ê°œìˆ˜
t = torch.tensor([[0,0,4,3],[5,6,0,0]])
print('Share: ', t.size())
print(t)
print('\n')

t.to_sparse_csc() # Dense Tensorë¥¼ CSR Sparse Tensor í˜•ì‹ìœ¼ë¡œ ë³€í™˜
```

    Share:  torch.Size([2, 4])
    tensor([[0, 0, 4, 3],
            [5, 6, 0, 0]])
    
    
    




    tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),
           row_indices=tensor([1, 1, 0, 0]),
           values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,
           layout=torch.sparse_csc)



2.2.3 CSR Soarse Tensor ìƒì„±


```python
# sparce_csr_tensor: CSR í˜•ì‹ì˜ Sparse tensorë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
crow_indices = torch.tensor([0,2,2])
col_indices = torch.tensor([0,1])
values = torch.tensor([1,2])
csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)

print(csr)
print('\n')
print(csr.to_dense())
```

    tensor(crow_indices=tensor([0, 2, 2]),
           col_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)
    
    
    tensor([[1, 2],
            [0, 0]])
    


```python
# sparce_csc_tensor: CSC í˜•ì‹ì˜ Sparse tensorë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
ccol_indices = torch.tensor([0,2,2])
row_indices = torch.tensor([0,1])
values = torch.tensor([1,2])
csc = torch.sparse_csc_tensor(ccol_indices = ccol_indices, row_indices = row_indices, values = values)

print(csr)
print('\n')
print(csc.to_dense())
```

    tensor(ccol_indices=tensor([0, 2, 2]),
           row_indices=tensor([0, 1]),
           values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csc)
    
    
    tensor([[1, 0],
            [2, 0]])
    

###2.3 Sparse Tensorì˜ í•„ìš”ì„± ì´í•´ ë° ì‹¤ìŠµ   

- ì•„ì£¼ í° í¬ê¸°ì˜ matrixë¥¼ êµ¬ì„±í•  ë•Œ, ì¼ë°˜ì ì¸ dense tensorëŠ” ë©”ë«¼ ì•„ì›ƒ í˜„ìƒì´ ë°œìƒí•˜ì§€ë§Œ, sparse tensorëŠ” ë©”ëª¨ë¦¬ ì•„ì›ƒ í˜„ìƒì´ ë°œìƒí•˜ì§€ ì•ŠìŒ


```python
# to_dense(): sparse tensorë¥¼ dense tensorë¡œ ë§Œë“œëŠ” í•¨ìˆ˜
i = torch.randint(0,100000,(200000,)).reshape(2,-1)
v = torch.rand(100000)
coo_sparse_tensor = torch.sparse_coo_tensor(indices = i, values = v, size = (100000,100000)) # COO Sparse Tensor (100000x100000)
coo_sparse_tensor
```




    tensor(indices=tensor([[ 8079, 96765, 24033,  ..., 74971, 86518, 39290],
                           [20287, 71308, 80797,  ..., 24146, 21949,  2927]]),
           values=tensor([0.0174, 0.4642, 0.3821,  ..., 0.4562, 0.5861, 0.9411]),
           size=(100000, 100000), nnz=100000, layout=torch.sparse_coo)




```python
crow = torch.randint(0,100000,(100000,))
col = torch.randint(0,100000,(100000,))
v = torch.rand(100000)
csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000x100000)
csr_sparse_tensor
```




    tensor(crow_indices=tensor([33096, 94883, 74398,  ...,  8843,  7862, 42789]),
           col_indices=tensor([ 9868, 84780,  2244,  ...,  3275, 76017, 21069]),
           values=tensor([0.0563, 0.0192, 0.9881,  ..., 0.1575, 0.0997, 0.6914]),
           size=(99999, 100000), nnz=100000, layout=torch.sparse_csr)




```python
# COO í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ Sparse Tensorë¥¼ Dense Tensorë¡œ ë³€í™˜, ë©”ëª¨ë¦¬ ì•„ì›ƒ
coo_sparse_tensor.to_dense() # ì„¸ì…˜ ë‹¤ìš´ë¨
```


```python
# ì»¤ë„ì„ ì¬ì‹œì‘ í•˜ê¸°ì— ë‹¤ì‹œ torch library ë¡œë“œ
import torch
```


```python
# CSR í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ Sparse Tensorë¥¼ Dense Tensorë¡œ ë³€í™˜, ë©”ëª¨ë¦¬ ì•„ì›ƒ
csr_sparse_tensor.to_dense() # ì„¸ì…˜ ë‹¤ìš´ë¨
```


```python
# ì»¤ë„ì„ ì¬ì‹œì‘ í•˜ê¸°ì— ë‹¤ì‹œ torch library ë¡œë“œ
import torch
```

###2.4 Sparse Tensorì˜ ì¡°ì‘ ì˜ˆì‹œ   


```python
# Sparseì™€ Sparse Tensor ê°„ì˜ ì—°ì‚° (2ì°¨ì›)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float)
b = torch.tensor([[1,0],[0,0]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a,b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())
print('\n')

print('ê³±ì…ˆ')
print(torch.mul(a,b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())
print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())
```

    ë§ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    ê³±ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    í–‰ë ¬ê³±
    tensor([[True, True],
            [True, True]])
    


```python
# Sparseì™€ Sparse Tensor ê°„ì˜ ì—°ì‚° (3ì°¨ì›)
a = torch.tensor([[[0,1],[0,2]],[[0,1],[0,2]]], dtype=torch.float)
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a,b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())
print('\n')

print('ê³±ì…ˆ')
print(torch.mul(a,b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())
print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # ì—ëŸ¬ ë°œìƒ. 3ì°¨ì›ë¼ë¦¬ëŠ” í–‰ë ¬ê³±ì´ ë¶ˆê°€ëŠ¥í•¨
```

    ë§ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    ê³±ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    í–‰ë ¬ê³±
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-4-55a52bea0e54> in <cell line: 17>()
         15 
         16 print('í–‰ë ¬ê³±')
    ---> 17 print(torch.matmul(a,b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    



```python
# Denseì™€ Sparse Tensor ê°„ì˜ ì—°ì‚°(2ì°¨ì›)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float)
b = torch.tensor([[1,0],[0,0]], dtype=torch.float)

sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a,b).to_dense() == torch.add(a, sparse_b).to_dense())
print('\n')

print('ê³±ì…ˆ')
print(torch.mul(a,b).to_dense() == torch.mul(a, sparse_b).to_dense())
print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense())
```

    ë§ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    ê³±ì…ˆ
    tensor([[True, True],
            [True, True]])
    
    
    í–‰ë ¬ê³±
    tensor([[True, True],
            [True, True]])
    


```python
# Denseì™€ Sparse Tensor ê°„ì˜ ì—°ì‚°(3ì°¨ì›)
a = torch.tensor([[[0,1],[0,2]],[[0,1],[0,2]]], dtype=torch.float)
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_b = b.to_sparse()

print('ë§ì…ˆ')
print(torch.add(a,b).to_dense() == torch.add(a, sparse_b).to_dense())
print('\n')

print('ê³±ì…ˆ')
print(torch.mul(a,b).to_dense() == torch.mul(a, sparse_b).to_dense())
print('\n')

print('í–‰ë ¬ê³±')
print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense()) # ì—ëŸ¬ ë°œìƒ
```

    ë§ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    ê³±ì…ˆ
    tensor([[[True, True],
             [True, True]],
    
            [[True, True],
             [True, True]]])
    
    
    í–‰ë ¬ê³±
    


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-7-26f355b228ca> in <cell line: 16>()
         14 
         15 print('í–‰ë ¬ê³±')
    ---> 16 print(torch.matmul(a,b).to_dense() == torch.matmul(a, sparse_b).to_dense())
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:43986 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26824 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:929 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:20475 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:23 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4733 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:15232 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16728 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:487 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:819 [kernel]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1077 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]
    



```python
# Sparse Tensorì˜ Indexing
a = torch.tensor([[0,1],[0,2]])
b = torch.tensor([[[1,0],[0,0]],[[1,0],[0,0]]], dtype=torch.float)

sparse_a = a.to_sparse()
sparse_b = b.to_sparse()

print('2ì°¨ì› Sparse Tensor ì¸ë±ì‹±')
print(a[0] == sparse_a[0].to_dense())
print('\n')

print('3ì°¨ì› Sparse Tensor ì¸ë±ì‹±')
print(b[0] == sparse_b[0].to_dense())
```

    2ì°¨ì› Sparse Tensor ì¸ë±ì‹±
    tensor([True, True])
    
    
    3ì°¨ì› Sparse Tensor ì¸ë±ì‹±
    tensor([[True, True],
            [True, True]])
    


```python
# 2ì°¨ì› Sparse Tensor(CSR)
a = torch.tensor([[0,1],[0,2]], dtype=torch.float).to_sparse_csr()
print(a[0].to_dense)
```

    <built-in method to_dense of Tensor object at 0x7abda8737ec0>
    


```python
a[0,:] # íŒŒì´í† ì¹˜ ë‚´ë¶€ì—ì„œ sparce tensorì— ëŒ€í•œ ìŠ¬ë¼ì´ì‹± êµ¬í˜„ì´ ì•ˆ ë˜ì–´ìˆê¸° ë•Œë¬¸
```


    ---------------------------------------------------------------------------

    NotImplementedError                       Traceback (most recent call last)

    <ipython-input-8-d9d9ecd2ddf9> in <cell line: 1>()
    ----> 1 a[0,:] # íŒŒì´í† ì¹˜ ë‚´ë¶€ì—ì„œ sparce tensorì— ëŒ€í•œ ìŠ¬ë¼ì´ì‹± êµ¬í˜„ì´ ì•ˆ ë˜ì–´ìˆê¸° ë•Œë¬¸
    

    NotImplementedError: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].
    
    CPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]
    CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]
    Meta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]
    QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]
    QuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]
    BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
    Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]
    FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]
    Functionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22129 [kernel]
    Named: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]
    Conjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]
    Negative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]
    ZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]
    ADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4930 [kernel]
    AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]
    Tracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16910 [kernel]
    AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]
    AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]
    FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]
    BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
    FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
    Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]
    VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
    FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]
    PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]
    FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]
    PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]
    PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]
    

