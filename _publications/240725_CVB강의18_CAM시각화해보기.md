# CVB강의18-CAM시각화해보기

**목차**
1. ResNet-18의 Grad-CAM 결과 확인하기   
  1.1 샘플 이미지와 ResNet-18 모델 불러오기   
  1.2 모델의 Grad-CAM 결과 구하기   
  1.3 Class에 따른 GradCAM 결과 시각화해보기   
  1.4 여러 Object가 있는 이미지에 대한 GradCAM 결과 시각화 해보기   
2. VGG-19의 Grad-CAM 결과 확인하기   
  2.1 VGG-19 모델 로드 및 GradCAM 계산   
  2.2 Grad-CAM 결과 시각화   

## 0. 환경설정


```python
!pip install torch==2.0.1 -q
!pip install torchvision==0.15.2 -q
!pip install gdown==4.6.6 -q
!curl -OL https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt
```

    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m619.9/619.9 MB[0m [31m3.0 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m317.1/317.1 MB[0m [31m3.7 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.8/11.8 MB[0m [31m84.1 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.0/21.0 MB[0m [31m73.2 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m849.3/849.3 kB[0m [31m42.7 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m557.1/557.1 MB[0m [31m3.3 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m168.4/168.4 MB[0m [31m7.7 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m54.6/54.6 MB[0m [31m12.8 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m102.6/102.6 MB[0m [31m8.1 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m173.2/173.2 MB[0m [31m5.3 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m177.1/177.1 MB[0m [31m5.9 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m98.6/98.6 kB[0m [31m6.0 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m63.3/63.3 MB[0m [31m11.2 MB/s[0m eta [36m0:00:00[0m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m96.4/96.4 kB[0m [31m6.5 MB/s[0m eta [36m0:00:00[0m
    [?25h[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.0.1 which is incompatible.[0m[31m
    [2K   [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.0/6.0 MB[0m [31m41.2 MB/s[0m eta [36m0:00:00[0m
    [?25h  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                     Dload  Upload   Total   Spent    Left  Speed
    100 10472  100 10472    0     0  63475      0 --:--:-- --:--:-- --:--:-- 63853
    


```python
import matplotlib.pyplot as plt
from PIL import Image
import PIL
import cv2
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F

import torchvision
import torchvision.transforms as T
```

## 1. ResNet-18의 Grad-CAM 결과 확인하기

### 1.1 샘플 이미지와 ResNet-18 모델 불러오기


```python
im = Image.open('/content/dog.jpg').resize((512,512))
plt.imshow(im)
```




    <matplotlib.image.AxesImage at 0x7beceb3905b0>




    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_7_1.png)
    



```python
# 불러온 이미지는 model의 input로 쓸 수 있도록 전처리함
preprocess = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256), # 이미지의 짧은 변이 256이 되도록 조
    torchvision.transforms.CenterCrop(224), # 이미지의 중앙 부분을 224x224 크기로 자름
    torchvision.transforms.ToTensor(), # 이미지를 PyTorch 텐서로 변환
    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # 이미지 정규화. RGB 각 채널에 대한 평균과 표준편차
])
```


```python
# torch.hub에서 원하는 모델 불러오고 가속환경 설정해주기
model = torch.hub.load('pytorch/vision', 'resnet18', weights = 'ResNet18_Weights.DEFAULT')
device = 'cpu'
model = model.to(device).eval()
im_tensor = preprocess(im).unsqueeze(0).to(device)
print(model)
```

    Downloading: "https://github.com/pytorch/vision/zipball/main" to /root/.cache/torch/hub/main.zip
    Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
    100%|██████████| 44.7M/44.7M [00:00<00:00, 131MB/s]
    

    ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer2): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer3): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (layer4): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): BasicBlock(
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=512, out_features=1000, bias=True)
    )
    

### 1.2 모델의 Grad-CAM 결과 구하기

`register_forward_hook`은 forward()함수가 계산될 때마다 불리게 되고, `register_backward_hook`은 target layer가 gradient가 존재하는 모듈인 경우 자동으로 불리게 됨


```python
class GradCam(nn.Module):
  def __init__(self, model, module, layer):
    super().__init__()
    self.model = model
    self.module = module
    self.layer = layer
    self.register_hooks()

  # 모델의 특정 레이어에 forward 및 backward 후크를 등록하여 Grad-CAM 활성화 맴을 생성하기 위한 기초작업을 수행함

  # register_forward_hook: Target Layer에 대해 forward pass를 통해 feature map를 얻음
  # register_backward_hook: Target Layer에 대해 backward pass를 통해 gradient를 계산
  def register_hooks(self):

    # _modules.items(): 모델의 모듈 이름과 모듈 자체를 반환
    for module_name, module in self.model._modules.items():
      if module_name == self.module:
        for layer_name, module in module._modules.items():
          if layer_name == self.layer:

            # 현재 모듈이 서브모듈을 포함하고 있는지 확인
            if len(module._modules.items()) != 0:

              # 맨 뒤에 있는 서브모듈(batchnorm layer)에 훅을 걺
              list(module._modules.items())[-1][1].register_forward_hook(self.forward_hook)
              list(module._modules.items())[-1][1].register_backward_hook(self.backward_hook)

              # 어떤 target layer에 훅이 걸렸는지 확인
              print(f'Target Layer is {module_name}-{layer_name}, {list(module._modules.items())[-1][1]}')

            # ResNet이 아닌 모델에서도 활용할 수 있도록 else문 처리함
            else:
              # 현재 모듈에 직접 훅을 걺
              module.register_forward_hook(self.forward_hook)
              module.register_backward_hook(self.backward_hook)
              print(f'Target Layer is {module_name}-{layer_name}')

  def forward(self, input, target_class):

    # 입력 데이터를 모델에 통과시켜 출력을 계산
    # outs.shape = [Batch, # of Class]
    outs = self.model(input)

    # 배치 차원을 제거하여 출력을 압축함. 배치크기가 1일 경우 불필요한 차원을 제거하기 위함
    # outs.shape = [# of Class]
    outs = outs.squeeze()

    # targe class option을 주지 않을 경우 가장 높은 score를 가진 class의 activation map을 시각화 함
    if target_class is None:
      target_class = outs.argmax()

    # target class에 대해서 backward pass를 진행함
    # retain_graph = True: 그래프를 유지하여 이후 추가적인 backward 연산을 수행할 수 있도록 함
    outs[target_class].backward(retain_graph=True)

    # 각 Channel마다 Gradient들의 평균을 구함
    # dim=(1,2): 그라디언트 텐서의 높이와 너비에 대해 평균을 계산
    # keepdim=True: 계산된 평균을 유지하여 텐서의 차원을 유지
    a_k = torch.mean(self.backward_result, dim=(1,2), keepdim=True)

    # Forward pass 결과와 Backward pass 결과로 구한 gradient(weight)를 곱해줌
    importance_weights = torch.sum(a_k * self.forward_result, dim=0)
    # ReLU 함수를 통해 음수를 제거해주고, 최댓값으로 나누어 [0,1]의 값을 가질 수 있도록 normalize 해줌
    # torch.max(importance_weights): 중요도 가중치의 최댓값. 이 값으로 나누어 정규화함
    activation_map = torch.relu(importance_weights) / torch.max(importance_weights)

    # 생성된 활성화맵, 모델의 출력결과, 모델의 예측 클래스 반환
    return activation_map, outs, outs.argmax()

  def forward_hook(self, _, input, output):
    self.forward_result = torch.squeeze(output)

  def backward_hook(self, _, grad_input, grad_output):
    self.backward_result = torch.squeeze(grad_output[0])
```


```python
# 맨 마지막 레이어(layer4)의 맨 마지막 블럭(layer='1')
grad_cam = GradCam(model=model, module='layer4', layer='1')
mask, outs, cls_pred = grad_cam(im_tensor, None)

# activation map의 size를 확인함
print(mask.shape, cls_pred.item())

# activation map을 시각화함
# mask.cpu(): 텐서를 cpu로 이동 / detach(): 텐서를 현재 계산 그래프에서 분리 / squeeze(): 불필요한 차원 제거 / numpy(): 텐서를 numpy 배열로 변환(matplotlib 이미지 시각화를 위해)
plt.imshow(mask.cpu().detach().squeeze().numpy())
```

    Target Layer is layer4-1, BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    

    /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
      warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
    

    torch.Size([7, 7]) 153
    




    <matplotlib.image.AxesImage at 0x7bece791b100>




    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_13_4.png)
    


### 1.3 Class에 따른 GradCAM 결과 시각화해보기


```python
def overlay_mask(img, mask):
  # 시각화를 위해 class activation map을 원본 이미지 크기로 변환시켜줌
  img_size = img.size
  mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), [img_size[1], img_size[0]], mode='bilinear')
  mask = mask.cpu().detach().squeeze().numpy()

  # 원본 이미지 위에 activation map을 그려줌
  plt.imshow(img)
  plt.imshow(mask, cmap='jet', alpha=0.5)
```


```python
with open('../content/imagenet_classes.txt', 'r') as f:
  classes = [s.strip() for s in f.readlines()]

# softmax 결과 가장 높은 확률을 가지는 3개의 class를 출력
probabilities = torch.nn.functional.softmax(outs, dim=0)
top3_prob, top3_classes = torch.topk(probabilities, 3)

fig = plt.figure(figsize=(20,20))
for i in range(top3_prob.size(0)):
  mask, _, _ = grad_cam(im_tensor, int(top3_classes[i].item()))

  a = fig.add_subplot(1,3,i+1)
  overlay_mask(im, mask)
  a.set_title(f'Class: {classes[top3_classes[i]]}, Score {top3_prob[i].item(): .4f}', fontsize=10)
```


    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_16_0.png)
    



```python
# softmax 결과 가장 낮은 확률을 가지는 3개의 class 출력
low_prob, low_classes = torch.topk(probabilities, 1000)

fig = plt.figure(figsize=(20,20))
for i in range(1,4):
  mask, _, _ = grad_cam(im_tensor, int(low_classes[-i].item()))

  a = fig.add_subplot(1,3,i)
  overlay_mask(im, mask)
  a.set_title(f'Class: {classes[low_classes[i]]}, Score {low_prob[i].item(): .4f}', fontsize=10)
```


    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_17_0.png)
    


### 1.4 여러 Object가 있는 이미지에 대한 GradCAM 결과 시각화 해보기


```python
# 여러 object가 있는 이미지 테스트 해보기
im2 = Image.open('/content/dog_cat.png').convert('RGB')
plt.imshow(im2)

im_tensor2 = preprocess(im2)
im_tensor2 = im_tensor2.unsqueeze(0).to(device) # 이미지를 batch 형태로 바꿔줌(BxCxWxH)
```


    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_19_0.png)
    



```python
# 마지막 Batch Normalization Layer에 대한 Grad-CAM을 구함
grad_cam = GradCam(model=model, module='layer4', layer='1')

# 207, golden_retriever class(class num=207)
mask_dog, outs, cls_pred = grad_cam(im_tensor2, 207)
# 282, tiger_cat class(class num=282)
mask_cat, outs, cls_pred = grad_cam(im_tensor2, 282)
# 319, dragonfly class(class num=319)
mask_dragonfly, outs, cls_pred = grad_cam(im_tensor2, 319)

masks = [(207, mask_dog),(282, mask_cat),(319, mask_dragonfly)]

fig = plt.figure(figsize=(20,20))
for i in range(len(masks)):
  a = fig.add_subplot(1,3, i+1)
  overlay_mask(im2, masks[i][1])
  a.set_title(f'Class: {classes[masks[i][0]]}', fontsize=10)
```

    Target Layer is layer4-1, BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    


    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_20_1.png)
    


## 2. VGG-19의 Grad-CAM 결과 확인하기

### 2.1 VGG-19 모델 로드 및 GradCAM 계산


```python
# 다른 모델에 대해서도 적용해보기
model = torch.hub.load('pytorch/vision', 'vgg19', weights = 'VGG19_Weights.DEFAULT')

device = 'cpu'
model = model.to(device).eval()
print(model)
```

    Using cache found in /root/.cache/torch/hub/pytorch_vision_main
    Downloading: "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth
    100%|██████████| 548M/548M [00:08<00:00, 71.4MB/s]
    

    VGG(
      (features): Sequential(
        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU(inplace=True)
        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (6): ReLU(inplace=True)
        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (8): ReLU(inplace=True)
        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU(inplace=True)
        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (13): ReLU(inplace=True)
        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (15): ReLU(inplace=True)
        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (17): ReLU(inplace=True)
        (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (20): ReLU(inplace=True)
        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (22): ReLU(inplace=True)
        (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (24): ReLU(inplace=True)
        (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (26): ReLU(inplace=True)
        (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (29): ReLU(inplace=True)
        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (31): ReLU(inplace=True)
        (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (33): ReLU(inplace=True)
        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (35): ReLU(inplace=True)
        (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
      (classifier): Sequential(
        (0): Linear(in_features=25088, out_features=4096, bias=True)
        (1): ReLU(inplace=True)
        (2): Dropout(p=0.5, inplace=False)
        (3): Linear(in_features=4096, out_features=4096, bias=True)
        (4): ReLU(inplace=True)
        (5): Dropout(p=0.5, inplace=False)
        (6): Linear(in_features=4096, out_features=1000, bias=True)
      )
    )
    


```python
# VGG19의 마지막 ReLU Layer에 대한 Grad-CAM 결과를 얻기
grad_cam = GradCam(model=model, module='features', layer='35')
mask, outs, cls_pred = grad_cam(im_tensor, None)
print(mask.shape, cls_pred.item())
plt.imshow(mask.detach().cpu().numpy())
```

    Target Layer is features-35
    torch.Size([14, 14]) 203
    




    <matplotlib.image.AxesImage at 0x7bece635d180>




    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_24_2.png)
    


### 2.2 Grad-CAM 결과 시각화


```python
# softmax 결과 가장 높은 확률을 가지는 3개의 class 출력
probabilities = torch.nn.functional.softmax(outs, dim=0)
top3_prob, top3_classes = torch.topk(probabilities, 3)

fig = plt.figure(figsize=(20,20))
for i in range(top3_prob.size(0)):
  mask, _, _ = grad_cam(im_tensor, int(top3_classes[i].item()))

  a = fig.add_subplot(1, 3, i+1)
  overlay_mask(im, mask)
  a.set_title(f'Class: {classes[top3_classes[i]]}, Score {top3_prob[i].item(): .4f}', fontsize=10)
```


    
![png](240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_files/240725_CVB%EA%B0%95%EC%9D%9818_CAM%EC%8B%9C%EA%B0%81%ED%99%94%ED%95%B4%EB%B3%B4%EA%B8%B0_26_0.png)
    

