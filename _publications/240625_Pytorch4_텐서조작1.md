# [온라인강의]Pytorch4_텐서조작(1)

### **~ 목차 ~**

## 0. PyTorch 설치 및 불러오기


```python
!pip install torch==2.0.1
```

    Collecting torch==2.0.1
      Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m619.9/619.9 MB[0m [31m1.4 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12.1)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)
    Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m21.0/21.0 MB[0m [31m55.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m849.3/849.3 kB[0m [31m56.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)
      Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m11.8/11.8 MB[0m [31m82.8 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)
      Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m557.1/557.1 MB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)
      Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m317.1/317.1 MB[0m [31m3.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)
      Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m168.4/168.4 MB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)
      Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m54.6/54.6 MB[0m [31m10.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)
      Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m102.6/102.6 MB[0m [31m9.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)
      Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m173.2/173.2 MB[0m [31m6.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)
      Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m177.1/177.1 MB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)
      Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m98.6/98.6 kB[0m [31m13.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting triton==2.0.0 (from torch==2.0.1)
      Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m63.3/63.3 MB[0m [31m9.6 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)
    Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)
    Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)
    Collecting lit (from triton==2.0.0->torch==2.0.1)
      Downloading lit-18.1.7-py3-none-any.whl (96 kB)
    [2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m96.4/96.4 kB[0m [31m13.8 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)
    Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)
    Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch
      Attempting uninstall: triton
        Found existing installation: triton 2.3.0
        Uninstalling triton-2.3.0:
          Successfully uninstalled triton-2.3.0
      Attempting uninstall: torch
        Found existing installation: torch 2.3.0+cu121
        Uninstalling torch-2.3.0+cu121:
          Successfully uninstalled torch-2.3.0+cu121
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.[0m[31m
    [0mSuccessfully installed lit-18.1.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0
    


```python
import torch
import numpy as np
import warnings
warnings.filterwarnings('ignore')
```

## 1. 텐서 이해하기

### 1.1 텐서를 생성하고 텐서로 변환하는 방법을 이해 및 실습

1.1.1 텐서의 값을 생성하는 방법들


```python
# rand: 0과 1사이의 균일한 분포(Uniform Distribution)에서 무작위로 생성된 텐서를 반환
torch.rand(2,3)
```




    tensor([[0.4458, 0.5729, 0.3091],
            [0.0996, 0.1764, 0.5964]])




```python
# randn: 평균이 0이고 표준편차가 1인 정규분포(가우시안 분포)에서 무작위로 생성된 텐서를 반환
torch.randn(2,3)
```




    tensor([[ 0.4346,  0.1743,  0.0090],
            [-0.1129, -0.4507, -1.1615]])




```python
# randint: 주어진 범위 내에서 정수값을 무작위로 선택하여 텐서를 생성(단 최솟값을 포함하고 최댓값은 포함하지 않음)
torch.randint(1,10,(5,5))
```




    tensor([[7, 8, 1, 2, 6],
            [7, 6, 2, 3, 5],
            [6, 5, 8, 1, 6],
            [3, 7, 8, 8, 7],
            [3, 6, 7, 3, 9]])




```python
# zeros: 모든 요소가 0인 텐서 반환
torch.zeros(3,3)
```




    tensor([[0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.]])




```python
# ones: 모든 요소가 1인 텐서 반환
torch.ones(2,2,2)
```




    tensor([[[1., 1.],
             [1., 1.]],
    
            [[1., 1.],
             [1., 1.]]])




```python
# full: 모든 요소가 지정된 값인 텐서 생성
torch.full((2,3),5)
```




    tensor([[5, 5, 5],
            [5, 5, 5]])




```python
# eye: 단위행렬 반환
torch.eye(3)
```




    tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])



1.1.2 다양한 데이터를 텐서 형식으로 변환하기


```python
# list, tuple, numpy array를 텐서로 바꾸기
ls = [[1,2,3,4,5],[6,7,8,9,10]]
tup = tuple([1,2,3])
arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])

print(torch.tensor(ls))
print('\n')
print(torch.tensor(tup))
print('\n')
print(torch.tensor(arr))
```

    tensor([[ 1,  2,  3,  4,  5],
            [ 6,  7,  8,  9, 10]])
    
    
    tensor([1, 2, 3])
    
    
    tensor([[[ 1,  2,  3],
             [ 4,  5,  6]],
    
            [[ 7,  8,  9],
             [10, 11, 12]]])
    


```python
# 이렇게도 바꿀 수 있음
torch.from_numpy(arr)
```




    tensor([[[ 1,  2,  3],
             [ 4,  5,  6]],
    
            [[ 7,  8,  9],
             [10, 11, 12]]])




```python
# as_tensor: 변환 전 데이터와의 메모리 공유(memory sharing)를 사용하므로 변환 전 데이터 변경시 변환되어 있는 텐서에도 반영됨

# torch.tensor와 torch.as_tensor의 차이점 알아보기
print('torch.tensor')
data1 = np.array([1,2,3,4,5])
tensor1 = torch.tensor(data1)
data1[0] = 10
print(tensor1) # 원본 데이터의 값 변경에 영향을 받지 않음

print('---------' * 3)

print('torch.as_tensor')
data2 = np.array([1,2,3,4,5])
tensor2 = torch.as_tensor(data2)
data2[0] = 10
print(tensor2) # 원본 데이터으 ㅣ값 변경에 영향을 받음
```

    torch.tensor
    tensor([1, 2, 3, 4, 5])
    ---------------------------
    torch.as_tensor
    tensor([10,  2,  3,  4,  5])
    


```python
# Tensor: float32 type으로 텐서 변환
data = [1,2,3,4,5]
tensor1 = torch.tensor(data) # 사실 torch.tensor(data, dtype=torch.float32) <- 이렇게 해도 됨
print('torch tensor')
print('Output:', tensor1)
print('Type', tensor1.dtype) # 원본의 데이터 타입을 그대로 따라감

print('--------' * 5)

tensor2 = torch.Tensor(data)
print('torch Tensor')
print('Output:', tensor2)
print('Type', tensor2.dtype) # float32 타입으로 텐서를 변환함
```

    torch tensor
    Output: tensor([1, 2, 3, 4, 5])
    Type torch.int64
    ----------------------------------------
    torch Tensor
    Output: tensor([1., 2., 3., 4., 5.])
    Type torch.float32
    

### 1.2 텐서에서의 indexing 이해 및 실습

- indexing: 텐서 내의 특정 요소를 index를 통해 접근할 수 있는 방법을 의미함


```python
# 1차원 텐서에서 Indexing 하기
tmp_1dim = torch.tensor([i for i in range(10)]) # 0부터 9까지의 값을 가지는 1차원 텐서 생성

print(tmp_1dim[0])
print(tmp_1dim[5])
print(tmp_1dim[-1])
```

    tensor(0)
    tensor(5)
    tensor(9)
    


```python
# 3차원 텐서에서 Indexing 하기
tmp_3dim = torch.randn(4,3,2)
print('Shape: ', tmp_3dim.shape)
print(tmp_3dim)

print('--------' * 5)

print(tmp_3dim[:,:,0].shape)
print(tmp_3dim[:,:,0])

print('\n')

print(tmp_3dim[0,:,1].shape)
print(tmp_3dim[0,:,1])
```

    Shape:  torch.Size([4, 3, 2])
    tensor([[[-0.3769,  0.9681],
             [ 0.1717,  0.4600],
             [ 1.3645,  0.4888]],
    
            [[ 0.8667, -0.7648],
             [ 1.1439,  2.0778],
             [ 0.2277, -0.8344]],
    
            [[ 0.1691,  0.2029],
             [ 0.1733,  0.6479],
             [ 1.0133, -0.2361]],
    
            [[ 0.3690,  0.4243],
             [-0.4031, -1.4049],
             [-0.3677,  0.0123]]])
    ----------------------------------------
    torch.Size([4, 3])
    tensor([[-0.3769,  0.1717,  1.3645],
            [ 0.8667,  1.1439,  0.2277],
            [ 0.1691,  0.1733,  1.0133],
            [ 0.3690, -0.4031, -0.3677]])
    
    
    torch.Size([3])
    tensor([0.9681, 0.4600, 0.4888])
    


```python
# index_select: 선택한 차원에서 인덱스에 해당하는 요소만 추출하는 함수
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

my_index = torch.tensor([0,2])
torch.index_select(tmp_2dim, dim=1, index=my_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    




    tensor([[ 0,  2],
            [10, 12]])




```python
# Masking을 이용한 Indexing: 조건에 따른 텐서의 요소를 사용하기 위한 방법으로 조건에 맞는 요소들만 반환하는 방법
# mask는 boolean 텐서임
mask = tmp_2dim >= 5
tmp_2dim[mask]
```




    tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])




```python
# masked_select: 주어진 mask에 해당하는 요소들을 추출하여 1차원으로 펼친 새로운 텐서를 반환하는 함수
torch.masked_select(tmp_2dim, mask = mask)
```




    tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])




```python
# take: 주어진 인덱스를 사용하여 텐서에서 요소를 선택하는 함수. 인덱스 번호는 텐서를 1차원으로 늘렸을 때를 기준으로 접근해야 함
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

my_index = torch.tensor([0,15])
torch.take(tmp_2dim, index = my_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    




    tensor([ 0, 15])



gather 조금 신기하다


```python
# gather: 주어진 차원에서 인덱스에 해당하는 요소들을 선택하여 새로운 텐서를 반환
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

recon_index = torch.tensor([[0,1],[9,8]])
print(recon_index)

print('\n')

dim = 1
torch.gather(tmp_2dim, dim = 1, index = recon_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    tensor([[0, 1],
            [9, 8]])
    
    
    




    tensor([[ 0,  1],
            [19, 18]])



## 2. 텐서의 모양 바꾸기

### 2.1 텐서의 shape를 바꾸는 여러가지 함수 이해 및 실습


```python
a = torch.randn(2,3,5)
a.size()
```




    torch.Size([2, 3, 5])




```python
a.shape # a.size()와 동일함
```




    torch.Size([2, 3, 5])




```python
# reshape: 텐서의 모양을 변경함. 메모리를 공유하지 않음

# 모양 변경
a = torch.randn(2,3,5)
print(a)
print('Shape: ', a.size())
print('\n')

reshape_a = a.reshape(5,6)
print(reshape_a)
print('Shape: ', reshape_a.size())
```

    tensor([[[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645],
             [ 0.3066,  0.8051, -0.7710, -0.6836,  0.2069],
             [ 0.6225,  2.3470, -0.7028,  0.8702, -0.7942]],
    
            [[ 0.7620, -1.3909, -0.2272,  0.4823,  0.6390],
             [ 0.2297, -0.4319,  1.2455, -0.9523,  0.0361],
             [ 0.1399,  0.8870,  1.2979, -0.3453,  0.2753]]])
    Shape:  torch.Size([2, 3, 5])
    
    
    tensor([[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645,  0.3066],
            [ 0.8051, -0.7710, -0.6836,  0.2069,  0.6225,  2.3470],
            [-0.7028,  0.8702, -0.7942,  0.7620, -1.3909, -0.2272],
            [ 0.4823,  0.6390,  0.2297, -0.4319,  1.2455, -0.9523],
            [ 0.0361,  0.1399,  0.8870,  1.2979, -0.3453,  0.2753]])
    Shape:  torch.Size([5, 6])
    


```python
# -1로 모양 자동 설정
reshape_auto_a = a.reshape(3,-1)
print(reshape_auto_a.size())
```

    torch.Size([3, 10])
    


```python
a.reshape(7,-1) # 30은 7로 나누어 떨어지지 않기 때문에 오류 발생
```


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-36-2ba3dea7cf49> in <cell line: 1>()
    ----> 1 a.reshape(7,-1)
    

    RuntimeError: shape '[7, -1]' is invalid for input of size 30



```python
# view: 텐서의 모양을 변경함
# reshape와 view가 따로 있는 이유는 뒤에서 다룰 예정임

print(a)
print('Shape: ', a.size())
print('\n')

view_a = a.view(5,6)
print(view_a)
print('Shape: ', view_a.size())
```

    tensor([[[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645],
             [ 0.3066,  0.8051, -0.7710, -0.6836,  0.2069],
             [ 0.6225,  2.3470, -0.7028,  0.8702, -0.7942]],
    
            [[ 0.7620, -1.3909, -0.2272,  0.4823,  0.6390],
             [ 0.2297, -0.4319,  1.2455, -0.9523,  0.0361],
             [ 0.1399,  0.8870,  1.2979, -0.3453,  0.2753]]])
    Shape:  torch.Size([2, 3, 5])
    
    
    tensor([[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645,  0.3066],
            [ 0.8051, -0.7710, -0.6836,  0.2069,  0.6225,  2.3470],
            [-0.7028,  0.8702, -0.7942,  0.7620, -1.3909, -0.2272],
            [ 0.4823,  0.6390,  0.2297, -0.4319,  1.2455, -0.9523],
            [ 0.0361,  0.1399,  0.8870,  1.2979, -0.3453,  0.2753]])
    Shape:  torch.Size([5, 6])
    


```python
view_auto_a = a.view(3,-1)
print(view_auto_a.size())
```

    torch.Size([3, 10])
    


```python
# transpose: 텐서의 차원을 전치함
tensor_a = torch.randint(1,10,(3,2,5))
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

# (3,2,5)를 (3,5,2)로 변경
trans_a = tensor_a.transpose(1,2) # 행과 열을 서로 전치, 서로 전치할 차원 2개를 지정
print(trans_a)
print('Shape: ',trans_a.size())
```

    tensor([[[3, 5, 1, 4, 4],
             [1, 5, 8, 2, 2]],
    
            [[9, 6, 3, 5, 6],
             [1, 6, 7, 5, 1]],
    
            [[3, 4, 6, 6, 6],
             [8, 2, 8, 4, 6]]])
    Shape:  torch.Size([3, 2, 5])
    
    
    tensor([[[3, 1],
             [5, 5],
             [1, 8],
             [4, 2],
             [4, 2]],
    
            [[9, 1],
             [6, 6],
             [3, 7],
             [5, 5],
             [6, 1]],
    
            [[3, 8],
             [4, 2],
             [6, 8],
             [6, 4],
             [6, 6]]])
    Shape:  torch.Size([3, 5, 2])
    


```python
# permute: 텐서 차원의 순서를 재배열함
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

permute_a = tensor_a.permute(0,2,1)
print(permute_a)
print('Shape: ', permute_a.size())
```

    tensor([[[3, 5, 1, 4, 4],
             [1, 5, 8, 2, 2]],
    
            [[9, 6, 3, 5, 6],
             [1, 6, 7, 5, 1]],
    
            [[3, 4, 6, 6, 6],
             [8, 2, 8, 4, 6]]])
    Shape:  torch.Size([3, 2, 5])
    
    
    tensor([[[3, 1],
             [5, 5],
             [1, 8],
             [4, 2],
             [4, 2]],
    
            [[9, 1],
             [6, 6],
             [3, 7],
             [5, 5],
             [6, 1]],
    
            [[3, 8],
             [4, 2],
             [6, 8],
             [6, 4],
             [6, 6]]])
    Shape:  torch.Size([3, 5, 2])
    

### 2.2 텐서의 차원을 추가하거나 변경하는 방법에 대한 이해 및 실습


```python
tensor_a = torch.tensor([i for i in range(10)]).reshape(5,2)
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

unsqu_a = tensor_a.unsqueeze(0) # 0번째 차원 하나 추가 (5,2) => (1,5,2)
print(unsqu_a)
print('Shape: ', unsqu_a.size())
```

    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])
    Shape:  torch.Size([5, 2])
    
    
    tensor([[[0, 1],
             [2, 3],
             [4, 5],
             [6, 7],
             [8, 9]]])
    Shape:  torch.Size([1, 5, 2])
    


```python
unsqu_a2 = tensor_a.unsqueeze(-1) # 마지막 번째에 차원 하나 추가 (5,2) => (5,2,1)
print(unsqu_a2)
print('Shape: ', unsqu_a2.size())
```

    tensor([[[0],
             [1]],
    
            [[2],
             [3]],
    
            [[4],
             [5]],
    
            [[6],
             [7]],
    
            [[8],
             [9]]])
    Shape:  torch.Size([5, 2, 1])
    


```python
# squeeze: 텐서에 차원의 크기가 1인 차원을 제거함
print(unsqu_a)
print('Shape: ', unsqu_a.size())
print('\n')

squ = unsqu_a.squeeze()
print(squ)
print('Shape: ', squ.size())
```

    tensor([[[0, 1],
             [2, 3],
             [4, 5],
             [6, 7],
             [8, 9]]])
    Shape:  torch.Size([1, 5, 2])
    
    
    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])
    Shape:  torch.Size([5, 2])
    


```python
x = torch.zeros(2,1,2,1,2)
print('Shape(original): ', x.size())
print('\n')

print('Shape(squeeze()): ', x.squeeze().size()) # 차원이 1인 모든 차원 제거
print('\n')

print('Shape(squeeze(0)): ', x.squeeze(0).size()) # 0번째 차원은 차원의 크기가 1이 아니므로 변화 없음
print('\n')

print('Shape(squeeze(1)): ', x.squeeze(1).size()) # 1번째 차원은 차원의 크기가 1이므로 제거
print('\n')

print('Shape(squeeze(0,1,3)): ', x.squeeze((0,1,3)).size()) # 여러 차원 제거 가능
```

    Shape(original):  torch.Size([2, 1, 2, 1, 2])
    
    
    Shape(squeeze()):  torch.Size([2, 2, 2])
    
    
    Shape(squeeze(0)):  torch.Size([2, 1, 2, 1, 2])
    
    
    Shape(squeeze(1)):  torch.Size([2, 2, 1, 2])
    
    
    Shape(squeeze(0,1,3)):  torch.Size([2, 2, 2])
    

- A텐서가 1차원일 경우: A텐서의 크기가 (m,)이면 m은 고정하고 (x,m)의 크기로만 확장 가능


```python
# expand: 텐서의 값을 반복하여 크기를 확장함
tensor_1dim = torch.tensor([1,2,3,4])
print(tensor_1dim)
print('Shape: ', tensor_1dim.size())
print('\n')

expand_tensor = tensor_1dim.expand(3,4)
print(expand_tensor)
print('Shape: ', expand_tensor.size())
```

    tensor([1, 2, 3, 4])
    Shape:  torch.Size([4])
    
    
    tensor([[1, 2, 3, 4],
            [1, 2, 3, 4],
            [1, 2, 3, 4]])
    Shape:  torch.Size([3, 4])
    

- A텐서가 2차원 이상일 경우: 크기가 1인 차원에 대해서만 적용 가능. A텐서의 크기가 (1,m)이면 (x,m), (m,1)이면 (m,y)로만 확장 가능


```python
tensor_2dim = torch.tensor([[1,2,3,4],[1,2,3,4]])
print(tensor_2dim)
print('Shape: ', tensor_2dim.size())
print('\n')

# 이렇게 하면 에러 발생함
expand_tensor = tensor_2dim.expand(4,4)
print(expand_tensor)
print('Shape: ', expand_tensor.size())
```

    tensor([[1, 2, 3, 4],
            [1, 2, 3, 4]])
    Shape:  torch.Size([2, 4])
    
    
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-51-fd068b76c84f> in <cell line: 7>()
          5 
          6 # 이렇게 하면 에러 발생함
    ----> 7 expand_tensor = tensor_2dim.expand(4,4)
          8 print(expand_tensor)
          9 print('Shape: ', expand_tensor.size())
    

    RuntimeError: The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 4].  Tensor sizes: [2, 4]



```python
# repeat: 텐서를 반복해서 크기를 확장함
# ex) A텐서가 (m,n)크기일 때 A텐서를 repeat(i,j)하면 결과값으로 (m*i, n*j)의 크기의 텐서가 생성됨
tensor_1dim = torch.tensor([1,2,3,4])
print(tensor_1dim)
print('Shape: ', tensor_1dim.size())
print('\n')

repeat_tensor = tensor_1dim.repeat(3,4)
print(repeat_tensor)
print('Shape: ', repeat_tensor.size())
```

    tensor([1, 2, 3, 4])
    Shape:  torch.Size([4])
    
    
    tensor([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],
            [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],
            [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])
    Shape:  torch.Size([3, 16])
    


```python
# flatten: 다차원 텐서를 1차원 텐서로 변경함
t = torch.tensor([i for i in range(20)]).reshape(2,5,2)
print(t)
print('Shape: ', t.size())
print('\n')

flat_tensor = t.flatten() # +) start_dim도 설정해줄 수 있음. 기본값은 0임
print(flat_tensor)
print('Shape: ', flat_tensor.size())
```

    tensor([[[ 0,  1],
             [ 2,  3],
             [ 4,  5],
             [ 6,  7],
             [ 8,  9]],
    
            [[10, 11],
             [12, 13],
             [14, 15],
             [16, 17],
             [18, 19]]])
    Shape:  torch.Size([2, 5, 2])
    
    
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
            18, 19])
    Shape:  torch.Size([20])
    


```python
# ravel: 다차원 텐서를 1차원 텐서로 변경함
t = torch.tensor([i for i in range(20)]).reshape(2,5,2)
print(t)
print('Shape: ', t.size())
print('\n')

ravel_tensor = t.ravel()
print(ravel_tensor)
print('Shape: ', ravel_tensor.size())
```

    tensor([[[ 0,  1],
             [ 2,  3],
             [ 4,  5],
             [ 6,  7],
             [ 8,  9]],
    
            [[10, 11],
             [12, 13],
             [14, 15],
             [16, 17],
             [18, 19]]])
    Shape:  torch.Size([2, 5, 2])
    
    
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
            18, 19])
    Shape:  torch.Size([20])
    


```python
t.ravel(1) # 에러 발생, ravel은 flatten과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-55-3baa156994b8> in <cell line: 1>()
    ----> 1 t.ravel(1) # 에러 발생, ravel은 flatten과 달리 어떠한 축을 기준으로 평탄화 하는 작업이 없음
    

    TypeError: _TensorBase.ravel() takes no arguments (1 given)


### 2.3 역할이 비슷한 함수들의 차이 이해 및 실습

2.3.1 모양 변경: view vs reshape vs unsqueeze   
- contiguous란?
  - 텐서의 메모리 상에 연속적인 데이터 배치를 갖는 것
  - 텐서를 청므 생성 후 정의하면 기본적으로 contiguous 하지만, 이에 대해 차원의 순서를 변경하는 과정을 거치면 contiguous하지 않아짐
  - 텐서의 contiguous함을 확인하기 위해서는 is_contiguous()를 사용하면 됨

- view: contiguous하지 않은 텐서에 대해서 동작하지 않음
- reshape: contiguous하지 않은 텐서를 contiguous하게 만들어주고 크기를 변경함
- unsqueeze: 차원의 크기가 1인 차원을 추가하지만 차원의 크기가 1이 아니면 차원의 모양을 변경할 수 없음


```python
# view vs reshape
tmp = torch.tensor([[[0,1],[2,3],[4,5]],
                    [[6,7],[8,9],[10,11]],
                    [[12,13],[14,15],[16,17]],
                    [[18,19],[20,21],[22,23]]])
tmp_t = tmp.transpose(0,1)
print(tmp_t.is_contiguous())
print(tmp_t.view(-1)) # 에러발생, view는 contiguous 하지 않은 텐서에 대해선 동작이 되지 않음
```

    False
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-56-c191427fbaf7> in <cell line: 5>()
          3 tmp_t = tmp.transpose(0,1)
          4 print(tmp_t.is_contiguous())
    ----> 5 print(tmp_t.view(-1))
    

    RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.



```python
reshape_tmp = tmp_t.reshape(-1) # reshape은 contiguous하지 않아도 동작이 됨
print(reshape_tmp)
print(reshape_tmp.is_contiguous()) # contiguous하지 않았던 Tensor를 contiguous하게 변경해줌
```

    tensor([ 0,  1,  6,  7, 12, 13, 18, 19,  2,  3,  8,  9, 14, 15, 20, 21,  4,  5,
            10, 11, 16, 17, 22, 23])
    True
    


```python
# (view, reshape) vs unsqueeze
tensor_a = torch.randn(2,3)

# (2,3)의 텐서를 (2,3,1)의 크기로 변경
view_tensor = tensor_a.view(2,3,1)
reshape_tensor = tensor_a.reshape(2,3,1)
unsqueeze_tensor = tensor_a.unsqueeze(-1) # unsqueeze는 비어있는 차원에 하나를 추가하는 모양변경이지 다른 모양변경을 할 순 없다 정도로 이해하면 됨

print('View output size: ', view_tensor.size())
print('Reshape output size: ', reshape_tensor.size())
print('Unsqueeze output size: ', unsqueeze_tensor.size())
```

    View output size:  torch.Size([2, 3, 1])
    Reshape output size:  torch.Size([2, 3, 1])
    Unsqueeze output size:  torch.Size([2, 3, 1])
    

2.3.2 차원변경: transpose vs permute
- transpose: 두 차원에 대해서만 변경 가능함
  - 인자가 총 2개여야 함
- permute: 모든 차원에 대해서 변경 가능함
  - 인자가 차원의 개수와 동일해야 함


```python
import torch
tensor_a = torch.randn(2,3,2)
transpose_tensor = tensor_a.transpose(2,1) # 행과 열을 전치
permute_tensor = tensor_a.permute(0,2,1) # 행과 열을 바꿈

print('Transpose tensor shape: ', transpose_tensor.size())
print('Permute tensor shape: ', permute_tensor.size())
```

    Transpose tensor shape:  torch.Size([2, 2, 3])
    Permute tensor shape:  torch.Size([2, 2, 3])
    


```python
tensor_a.permute(1,2,0).shape # permute는 이렇게도 가능함
```




    torch.Size([3, 2, 2])




```python
tensor_a.transpose(2,1).transpose(0,2).shape # transpose는 이렇게 두 번에 걸쳐서 해야 함
```




    torch.Size([3, 2, 2])



2.3.3 반복을 통한 텐서 크기 확장: expand vs repeat
- expand
  - 원본 텐서와 메모리를 공유함
- repeat
  - 원본 텐서와 메모리를 공유하지 않음


```python
# 원본 텐서 생성
tensor_a = torch.rand(1,1,3)
print('Original Tensor Size')
print(tensor_a.size())
print(tensor_a)
print('\n')

# expand 사용하여 (1,1,3) -> (4,1,3)
expand_tensor = tensor_a.expand(4,1,-1)
print('Shape of expanded tensor: ', expand_tensor.size())
print('\n')

# repeat 사용하여 (1,1,3) -> (4,1,3)
repeat_tensor = tensor_a.repeat(4,1,1)
print('Shape of repeated tensor: ', repeat_tensor.size())
print('\n')

# 평면화된 뷰 수정 후 원본 텐서 확인
tensor_a[:] = 0

print('Expanded Tensor')
print(expand_tensor) # 값 변경 됨
print('\n')

print('Repeated Tensor')
print(repeat_tensor) # 값 변경 안 됨
```

    Original Tensor Size
    torch.Size([1, 1, 3])
    tensor([[[0.6953, 0.8855, 0.1160]]])
    
    
    Shape of expanded tensor:  torch.Size([4, 1, 3])
    
    
    Shape of repeated tensor:  torch.Size([4, 1, 3])
    
    
    Expanded Tensor
    tensor([[[0., 0., 0.]],
    
            [[0., 0., 0.]],
    
            [[0., 0., 0.]],
    
            [[0., 0., 0.]]])
    
    
    Repeated Tensor
    tensor([[[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]]])
    

## 3. 텐서 합치기와 나누기

### 3.1 여러 텐서를 합치는 방법에 대한 이해 및 실습


```python
# cat: 주어진 차원을 따라 텐서들을 연결함 (주어진 차원 외의 다른 차원의 크기가 같아야 함)
tensor_a = torch.randint(1,10,(2,3))
tensor_b = torch.rand(5,3)

print('Tensor A shape: ', tensor_a.size())
print(tensor_a)
print('\n')

print('Tensor B shape: ', tensor_b.size())
print(tensor_b)
print('\n')

a_cat_b_row = torch.cat((tensor_a, tensor_b), dim=0) # dim=0 (행)
print('Concat Tensor A and B (by row) Shape: ', a_cat_b_row.shape) # (TensorA의 행 개수 + TensorB의 행 개수, Tensor A/B의 열 개수)
print(a_cat_b_row)
```

    Tensor A shape:  torch.Size([2, 3])
    tensor([[2, 3, 9],
            [6, 9, 7]])
    
    
    Tensor B shape:  torch.Size([5, 3])
    tensor([[0.6792, 0.6251, 0.3015],
            [0.9122, 0.3750, 0.3705],
            [0.2549, 0.9658, 0.6880],
            [0.4726, 0.5449, 0.4260],
            [0.4259, 0.7678, 0.3151]])
    
    
    Concat Tensor A and B (by row) Shape:  torch.Size([7, 3])
    tensor([[2.0000, 3.0000, 9.0000],
            [6.0000, 9.0000, 7.0000],
            [0.6792, 0.6251, 0.3015],
            [0.9122, 0.3750, 0.3705],
            [0.2549, 0.9658, 0.6880],
            [0.4726, 0.5449, 0.4260],
            [0.4259, 0.7678, 0.3151]])
    


```python
# stack: 주어진 차원을 새로운 차원으로 추가하여 텐서들을 쌓음
tensor_a = torch.randint(1,10,(3,2))
tensor_b = torch.rand(3,2)

print('Tensor A shape: ', tensor_a.size())
print(tensor_a)
print('\n')

print('Tensor B shape: ', tensor_b.size())
print(tensor_b)
print('\n')

stack_tensor_row = torch.stack([tensor_a, tensor_b], dim=0) # dim=0, 행을 기준으로 TensorA에 TensorB 쌓기
print('Stack A and B (by row): ', stack_tensor_row.size()) # 쌓은 Tensor 개수, Tensor A/B 행 계수, Tensor A/B 열 개수)
print(stack_tensor_row)
```

    Tensor A shape:  torch.Size([3, 2])
    tensor([[3, 2],
            [9, 9],
            [6, 8]])
    
    
    Tensor B shape:  torch.Size([3, 2])
    tensor([[0.3521, 0.6629],
            [0.2885, 0.1568],
            [0.0380, 0.5313]])
    
    
    Stack A and B (by row):  torch.Size([2, 3, 2])
    tensor([[[3.0000, 2.0000],
             [9.0000, 9.0000],
             [6.0000, 8.0000]],
    
            [[0.3521, 0.6629],
             [0.2885, 0.1568],
             [0.0380, 0.5313]]])
    

### 3.2 하나의 텐서를 여러개로 나누는 방법에 대한 이해 및 실습


```python
# chunk: 나눈고자 하는 텐서의 개수를 지정하여 원래의 텐서를 개수에 맞게 분리함
tensor_a = torch.randint(1,10,(6,4))
print('Original: ', tensor_a)
print('\n')

chunk_num = 3
chunk_tensor = torch.chunk(tensor_a, chunks = chunk_num, dim=0)
print(f'{len(chunk_tensor)} 개의 Tensor로 분리')
print('\n')

for idx,a in enumerate(chunk_tensor):
  print(f'{idx}번째 Tensor \n{a}')
  print(f'{idx}번째 Tensor 크기', a.size())
  print('----' * 10)
```

    Original:  tensor([[4, 4, 1, 4],
            [5, 2, 4, 7],
            [6, 6, 6, 1],
            [7, 9, 2, 1],
            [7, 8, 4, 8],
            [4, 5, 7, 8]])
    
    
    3 개의 Tensor로 분리
    
    
    0번째 Tensor 
    tensor([[4, 4, 1, 4],
            [5, 2, 4, 7]])
    0번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    1번째 Tensor 
    tensor([[6, 6, 6, 1],
            [7, 9, 2, 1]])
    1번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    2번째 Tensor 
    tensor([[7, 8, 4, 8],
            [4, 5, 7, 8]])
    2번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    


```python
# split: 입력한 크기로 여러 개의 작은 텐서로 나눔
tensor_a = torch.randint(1,10,(6,4))
print(tensor_a)
print('\n')

split_size = 2
split_tensor = torch.split(tensor_a, split_size_or_sections = split_size, dim = 0) # dim=0(행), 텐서A를 행의 길이가 2(split_size)인 텐서로 나눔
print(f'{len(split_tensor)}개의 Tensor로 분리')
print('\n')

for idx,a in enumerate(split_tensor):
  print(f'{idx}번째 Tensor \n{a}')
  print(f'{idx}번째 Tensor 크기', a.size())
  print('----' * 10)
```

    tensor([[4, 4, 9, 6],
            [2, 7, 1, 7],
            [3, 5, 9, 4],
            [7, 5, 8, 3],
            [8, 4, 1, 2],
            [7, 9, 8, 8]])
    
    
    3개의 Tensor로 분리
    
    
    0번째 Tensor 
    tensor([[4, 4, 9, 6],
            [2, 7, 1, 7]])
    0번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    1번째 Tensor 
    tensor([[3, 5, 9, 4],
            [7, 5, 8, 3]])
    1번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    2번째 Tensor 
    tensor([[8, 4, 1, 2],
            [7, 9, 8, 8]])
    2번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    


```python
# split_size의 입력으로 리스트를 넣을 수도 있음
tensor_a = torch.randint(1,10,(6,4))
print('Original: ', tensor_a)
print('\n')

split_size = [2,4]
split_tensor = torch.split(tensor_a, split_size_or_sections = split_size, dim = 0) # dim=0(행), 텐서A를 행의 길이가 2(split_size)인 텐서로 나눔
print(f'{len(split_tensor)}개의 Tensor로 분리')
print('\n')

for idx,a in enumerate(split_tensor):
  print(f'{idx}번째 Tensor \n{a}')
  print(f'{idx}번째 Tensor 크기', a.size())
  print('----' * 10)
```

    Original:  tensor([[7, 1, 7, 6],
            [5, 9, 4, 6],
            [6, 9, 6, 6],
            [3, 4, 7, 9],
            [7, 5, 7, 5],
            [2, 8, 8, 9]])
    
    
    2개의 Tensor로 분리
    
    
    0번째 Tensor 
    tensor([[7, 1, 7, 6],
            [5, 9, 4, 6]])
    0번째 Tensor 크기 torch.Size([2, 4])
    ----------------------------------------
    1번째 Tensor 
    tensor([[6, 9, 6, 6],
            [3, 4, 7, 9],
            [7, 5, 7, 5],
            [2, 8, 8, 9]])
    1번째 Tensor 크기 torch.Size([4, 4])
    ----------------------------------------
    
