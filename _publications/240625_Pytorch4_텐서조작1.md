# [ì˜¨ë¼ì¸ê°•ì˜]Pytorch4_í…ì„œì¡°ì‘(1)

### **~ ëª©ì°¨ ~**

## 0. PyTorch ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°


```python
!pip install torch==2.0.1
```

    Collecting torch==2.0.1
      Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m619.9/619.9 MB[0m [31m1.4 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.15.3)
    Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)
    Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12.1)
    Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.3)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)
    Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.0/21.0 MB[0m [31m55.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)
      Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m849.3/849.3 kB[0m [31m56.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)
      Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.8/11.8 MB[0m [31m82.8 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)
      Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m557.1/557.1 MB[0m [31m2.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)
      Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m317.1/317.1 MB[0m [31m3.3 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)
      Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m168.4/168.4 MB[0m [31m2.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)
      Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m54.6/54.6 MB[0m [31m10.9 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)
      Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m102.6/102.6 MB[0m [31m9.1 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)
      Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m173.2/173.2 MB[0m [31m6.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)
      Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m177.1/177.1 MB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)
      Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m98.6/98.6 kB[0m [31m13.6 MB/s[0m eta [36m0:00:00[0m
    [?25hCollecting triton==2.0.0 (from torch==2.0.1)
      Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m63.3/63.3 MB[0m [31m9.6 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (67.7.2)
    Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.43.0)
    Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.9)
    Collecting lit (from triton==2.0.0->torch==2.0.1)
      Downloading lit-18.1.7-py3-none-any.whl (96 kB)
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m96.4/96.4 kB[0m [31m13.8 MB/s[0m eta [36m0:00:00[0m
    [?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.5)
    Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)
    Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch
      Attempting uninstall: triton
        Found existing installation: triton 2.3.0
        Uninstalling triton-2.3.0:
          Successfully uninstalled triton-2.3.0
      Attempting uninstall: torch
        Found existing installation: torch 2.3.0+cu121
        Uninstalling torch-2.3.0+cu121:
          Successfully uninstalled torch-2.3.0+cu121
    [31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.[0m[31m
    [0mSuccessfully installed lit-18.1.7 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0
    


```python
import torch
import numpy as np
import warnings
warnings.filterwarnings('ignore')
```

## 1. í…ì„œ ì´í•´í•˜ê¸°

### 1.1 í…ì„œë¥¼ ìƒì„±í•˜ê³  í…ì„œë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì´í•´ ë° ì‹¤ìŠµ

1.1.1 í…ì„œì˜ ê°’ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ë“¤


```python
# rand: 0ê³¼ 1ì‚¬ì´ì˜ ê· ì¼í•œ ë¶„í¬(Uniform Distribution)ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒì„±ëœ í…ì„œë¥¼ ë°˜í™˜
torch.rand(2,3)
```




    tensor([[0.4458, 0.5729, 0.3091],
            [0.0996, 0.1764, 0.5964]])




```python
# randn: í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 1ì¸ ì •ê·œë¶„í¬(ê°€ìš°ì‹œì•ˆ ë¶„í¬)ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒì„±ëœ í…ì„œë¥¼ ë°˜í™˜
torch.randn(2,3)
```




    tensor([[ 0.4346,  0.1743,  0.0090],
            [-0.1129, -0.4507, -1.1615]])




```python
# randint: ì£¼ì–´ì§„ ë²”ìœ„ ë‚´ì—ì„œ ì •ìˆ˜ê°’ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ í…ì„œë¥¼ ìƒì„±(ë‹¨ ìµœì†Ÿê°’ì„ í¬í•¨í•˜ê³  ìµœëŒ“ê°’ì€ í¬í•¨í•˜ì§€ ì•ŠìŒ)
torch.randint(1,10,(5,5))
```




    tensor([[7, 8, 1, 2, 6],
            [7, 6, 2, 3, 5],
            [6, 5, 8, 1, 6],
            [3, 7, 8, 8, 7],
            [3, 6, 7, 3, 9]])




```python
# zeros: ëª¨ë“  ìš”ì†Œê°€ 0ì¸ í…ì„œ ë°˜í™˜
torch.zeros(3,3)
```




    tensor([[0., 0., 0.],
            [0., 0., 0.],
            [0., 0., 0.]])




```python
# ones: ëª¨ë“  ìš”ì†Œê°€ 1ì¸ í…ì„œ ë°˜í™˜
torch.ones(2,2,2)
```




    tensor([[[1., 1.],
             [1., 1.]],
    
            [[1., 1.],
             [1., 1.]]])




```python
# full: ëª¨ë“  ìš”ì†Œê°€ ì§€ì •ëœ ê°’ì¸ í…ì„œ ìƒì„±
torch.full((2,3),5)
```




    tensor([[5, 5, 5],
            [5, 5, 5]])




```python
# eye: ë‹¨ìœ„í–‰ë ¬ ë°˜í™˜
torch.eye(3)
```




    tensor([[1., 0., 0.],
            [0., 1., 0.],
            [0., 0., 1.]])



1.1.2 ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸°


```python
# list, tuple, numpy arrayë¥¼ í…ì„œë¡œ ë°”ê¾¸ê¸°
ls = [[1,2,3,4,5],[6,7,8,9,10]]
tup = tuple([1,2,3])
arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])

print(torch.tensor(ls))
print('\n')
print(torch.tensor(tup))
print('\n')
print(torch.tensor(arr))
```

    tensor([[ 1,  2,  3,  4,  5],
            [ 6,  7,  8,  9, 10]])
    
    
    tensor([1, 2, 3])
    
    
    tensor([[[ 1,  2,  3],
             [ 4,  5,  6]],
    
            [[ 7,  8,  9],
             [10, 11, 12]]])
    


```python
# ì´ë ‡ê²Œë„ ë°”ê¿€ ìˆ˜ ìˆìŒ
torch.from_numpy(arr)
```




    tensor([[[ 1,  2,  3],
             [ 4,  5,  6]],
    
            [[ 7,  8,  9],
             [10, 11, 12]]])




```python
# as_tensor: ë³€í™˜ ì „ ë°ì´í„°ì™€ì˜ ë©”ëª¨ë¦¬ ê³µìœ (memory sharing)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€í™˜ ì „ ë°ì´í„° ë³€ê²½ì‹œ ë³€í™˜ë˜ì–´ ìˆëŠ” í…ì„œì—ë„ ë°˜ì˜ë¨

# torch.tensorì™€ torch.as_tensorì˜ ì°¨ì´ì  ì•Œì•„ë³´ê¸°
print('torch.tensor')
data1 = np.array([1,2,3,4,5])
tensor1 = torch.tensor(data1)
data1[0] = 10
print(tensor1) # ì›ë³¸ ë°ì´í„°ì˜ ê°’ ë³€ê²½ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ

print('---------' * 3)

print('torch.as_tensor')
data2 = np.array([1,2,3,4,5])
tensor2 = torch.as_tensor(data2)
data2[0] = 10
print(tensor2) # ì›ë³¸ ë°ì´í„°ìœ¼ ã…£ê°’ ë³€ê²½ì— ì˜í–¥ì„ ë°›ìŒ
```

    torch.tensor
    tensor([1, 2, 3, 4, 5])
    ---------------------------
    torch.as_tensor
    tensor([10,  2,  3,  4,  5])
    


```python
# Tensor: float32 typeìœ¼ë¡œ í…ì„œ ë³€í™˜
data = [1,2,3,4,5]
tensor1 = torch.tensor(data) # ì‚¬ì‹¤ torch.tensor(data, dtype=torch.float32) <- ì´ë ‡ê²Œ í•´ë„ ë¨
print('torch tensor')
print('Output:', tensor1)
print('Type', tensor1.dtype) # ì›ë³¸ì˜ ë°ì´í„° íƒ€ì…ì„ ê·¸ëŒ€ë¡œ ë”°ë¼ê°

print('--------' * 5)

tensor2 = torch.Tensor(data)
print('torch Tensor')
print('Output:', tensor2)
print('Type', tensor2.dtype) # float32 íƒ€ì…ìœ¼ë¡œ í…ì„œë¥¼ ë³€í™˜í•¨
```

    torch tensor
    Output: tensor([1, 2, 3, 4, 5])
    Type torch.int64
    ----------------------------------------
    torch Tensor
    Output: tensor([1., 2., 3., 4., 5.])
    Type torch.float32
    

### 1.2 í…ì„œì—ì„œì˜ indexing ì´í•´ ë° ì‹¤ìŠµ

- indexing: í…ì„œ ë‚´ì˜ íŠ¹ì • ìš”ì†Œë¥¼ indexë¥¼ í†µí•´ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì˜ë¯¸í•¨


```python
# 1ì°¨ì› í…ì„œì—ì„œ Indexing í•˜ê¸°
tmp_1dim = torch.tensor([i for i in range(10)]) # 0ë¶€í„° 9ê¹Œì§€ì˜ ê°’ì„ ê°€ì§€ëŠ” 1ì°¨ì› í…ì„œ ìƒì„±

print(tmp_1dim[0])
print(tmp_1dim[5])
print(tmp_1dim[-1])
```

    tensor(0)
    tensor(5)
    tensor(9)
    


```python
# 3ì°¨ì› í…ì„œì—ì„œ Indexing í•˜ê¸°
tmp_3dim = torch.randn(4,3,2)
print('Shape: ', tmp_3dim.shape)
print(tmp_3dim)

print('--------' * 5)

print(tmp_3dim[:,:,0].shape)
print(tmp_3dim[:,:,0])

print('\n')

print(tmp_3dim[0,:,1].shape)
print(tmp_3dim[0,:,1])
```

    Shape:  torch.Size([4, 3, 2])
    tensor([[[-0.3769,  0.9681],
             [ 0.1717,  0.4600],
             [ 1.3645,  0.4888]],
    
            [[ 0.8667, -0.7648],
             [ 1.1439,  2.0778],
             [ 0.2277, -0.8344]],
    
            [[ 0.1691,  0.2029],
             [ 0.1733,  0.6479],
             [ 1.0133, -0.2361]],
    
            [[ 0.3690,  0.4243],
             [-0.4031, -1.4049],
             [-0.3677,  0.0123]]])
    ----------------------------------------
    torch.Size([4, 3])
    tensor([[-0.3769,  0.1717,  1.3645],
            [ 0.8667,  1.1439,  0.2277],
            [ 0.1691,  0.1733,  1.0133],
            [ 0.3690, -0.4031, -0.3677]])
    
    
    torch.Size([3])
    tensor([0.9681, 0.4600, 0.4888])
    


```python
# index_select: ì„ íƒí•œ ì°¨ì›ì—ì„œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

my_index = torch.tensor([0,2])
torch.index_select(tmp_2dim, dim=1, index=my_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    




    tensor([[ 0,  2],
            [10, 12]])




```python
# Maskingì„ ì´ìš©í•œ Indexing: ì¡°ê±´ì— ë”°ë¥¸ í…ì„œì˜ ìš”ì†Œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ì¡°ê±´ì— ë§ëŠ” ìš”ì†Œë“¤ë§Œ ë°˜í™˜í•˜ëŠ” ë°©ë²•
# maskëŠ” boolean í…ì„œì„
mask = tmp_2dim >= 5
tmp_2dim[mask]
```




    tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])




```python
# masked_select: ì£¼ì–´ì§„ maskì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œë“¤ì„ ì¶”ì¶œí•˜ì—¬ 1ì°¨ì›ìœ¼ë¡œ í¼ì¹œ ìƒˆë¡œìš´ í…ì„œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
torch.masked_select(tmp_2dim, mask = mask)
```




    tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])




```python
# take: ì£¼ì–´ì§„ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œì—ì„œ ìš”ì†Œë¥¼ ì„ íƒí•˜ëŠ” í•¨ìˆ˜. ì¸ë±ìŠ¤ ë²ˆí˜¸ëŠ” í…ì„œë¥¼ 1ì°¨ì›ìœ¼ë¡œ ëŠ˜ë ¸ì„ ë•Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•¨
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

my_index = torch.tensor([0,15])
torch.take(tmp_2dim, index = my_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    




    tensor([ 0, 15])



gather ì¡°ê¸ˆ ì‹ ê¸°í•˜ë‹¤


```python
# gather: ì£¼ì–´ì§„ ì°¨ì›ì—ì„œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìš”ì†Œë“¤ì„ ì„ íƒí•˜ì—¬ ìƒˆë¡œìš´ í…ì„œë¥¼ ë°˜í™˜
tmp_2dim = torch.tensor([[i for i in range(10)], [i for i in range(10,20)]])
print(tmp_2dim)

print('\n')

recon_index = torch.tensor([[0,1],[9,8]])
print(recon_index)

print('\n')

dim = 1
torch.gather(tmp_2dim, dim = 1, index = recon_index)
```

    tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])
    
    
    tensor([[0, 1],
            [9, 8]])
    
    
    




    tensor([[ 0,  1],
            [19, 18]])



## 2. í…ì„œì˜ ëª¨ì–‘ ë°”ê¾¸ê¸°

### 2.1 í…ì„œì˜ shapeë¥¼ ë°”ê¾¸ëŠ” ì—¬ëŸ¬ê°€ì§€ í•¨ìˆ˜ ì´í•´ ë° ì‹¤ìŠµ


```python
a = torch.randn(2,3,5)
a.size()
```




    torch.Size([2, 3, 5])




```python
a.shape # a.size()ì™€ ë™ì¼í•¨
```




    torch.Size([2, 3, 5])




```python
# reshape: í…ì„œì˜ ëª¨ì–‘ì„ ë³€ê²½í•¨. ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ì§€ ì•ŠìŒ

# ëª¨ì–‘ ë³€ê²½
a = torch.randn(2,3,5)
print(a)
print('Shape: ', a.size())
print('\n')

reshape_a = a.reshape(5,6)
print(reshape_a)
print('Shape: ', reshape_a.size())
```

    tensor([[[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645],
             [ 0.3066,  0.8051, -0.7710, -0.6836,  0.2069],
             [ 0.6225,  2.3470, -0.7028,  0.8702, -0.7942]],
    
            [[ 0.7620, -1.3909, -0.2272,  0.4823,  0.6390],
             [ 0.2297, -0.4319,  1.2455, -0.9523,  0.0361],
             [ 0.1399,  0.8870,  1.2979, -0.3453,  0.2753]]])
    Shape:  torch.Size([2, 3, 5])
    
    
    tensor([[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645,  0.3066],
            [ 0.8051, -0.7710, -0.6836,  0.2069,  0.6225,  2.3470],
            [-0.7028,  0.8702, -0.7942,  0.7620, -1.3909, -0.2272],
            [ 0.4823,  0.6390,  0.2297, -0.4319,  1.2455, -0.9523],
            [ 0.0361,  0.1399,  0.8870,  1.2979, -0.3453,  0.2753]])
    Shape:  torch.Size([5, 6])
    


```python
# -1ë¡œ ëª¨ì–‘ ìë™ ì„¤ì •
reshape_auto_a = a.reshape(3,-1)
print(reshape_auto_a.size())
```

    torch.Size([3, 10])
    


```python
a.reshape(7,-1) # 30ì€ 7ë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— ì˜¤ë¥˜ ë°œìƒ
```


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-36-2ba3dea7cf49> in <cell line: 1>()
    ----> 1 a.reshape(7,-1)
    

    RuntimeError: shape '[7, -1]' is invalid for input of size 30



```python
# view: í…ì„œì˜ ëª¨ì–‘ì„ ë³€ê²½í•¨
# reshapeì™€ viewê°€ ë”°ë¡œ ìˆëŠ” ì´ìœ ëŠ” ë’¤ì—ì„œ ë‹¤ë£° ì˜ˆì •ì„

print(a)
print('Shape: ', a.size())
print('\n')

view_a = a.view(5,6)
print(view_a)
print('Shape: ', view_a.size())
```

    tensor([[[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645],
             [ 0.3066,  0.8051, -0.7710, -0.6836,  0.2069],
             [ 0.6225,  2.3470, -0.7028,  0.8702, -0.7942]],
    
            [[ 0.7620, -1.3909, -0.2272,  0.4823,  0.6390],
             [ 0.2297, -0.4319,  1.2455, -0.9523,  0.0361],
             [ 0.1399,  0.8870,  1.2979, -0.3453,  0.2753]]])
    Shape:  torch.Size([2, 3, 5])
    
    
    tensor([[-0.3838, -0.6531,  0.2228,  0.1026, -0.2645,  0.3066],
            [ 0.8051, -0.7710, -0.6836,  0.2069,  0.6225,  2.3470],
            [-0.7028,  0.8702, -0.7942,  0.7620, -1.3909, -0.2272],
            [ 0.4823,  0.6390,  0.2297, -0.4319,  1.2455, -0.9523],
            [ 0.0361,  0.1399,  0.8870,  1.2979, -0.3453,  0.2753]])
    Shape:  torch.Size([5, 6])
    


```python
view_auto_a = a.view(3,-1)
print(view_auto_a.size())
```

    torch.Size([3, 10])
    


```python
# transpose: í…ì„œì˜ ì°¨ì›ì„ ì „ì¹˜í•¨
tensor_a = torch.randint(1,10,(3,2,5))
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

# (3,2,5)ë¥¼ (3,5,2)ë¡œ ë³€ê²½
trans_a = tensor_a.transpose(1,2) # í–‰ê³¼ ì—´ì„ ì„œë¡œ ì „ì¹˜, ì„œë¡œ ì „ì¹˜í•  ì°¨ì› 2ê°œë¥¼ ì§€ì •
print(trans_a)
print('Shape: ',trans_a.size())
```

    tensor([[[3, 5, 1, 4, 4],
             [1, 5, 8, 2, 2]],
    
            [[9, 6, 3, 5, 6],
             [1, 6, 7, 5, 1]],
    
            [[3, 4, 6, 6, 6],
             [8, 2, 8, 4, 6]]])
    Shape:  torch.Size([3, 2, 5])
    
    
    tensor([[[3, 1],
             [5, 5],
             [1, 8],
             [4, 2],
             [4, 2]],
    
            [[9, 1],
             [6, 6],
             [3, 7],
             [5, 5],
             [6, 1]],
    
            [[3, 8],
             [4, 2],
             [6, 8],
             [6, 4],
             [6, 6]]])
    Shape:  torch.Size([3, 5, 2])
    


```python
# permute: í…ì„œ ì°¨ì›ì˜ ìˆœì„œë¥¼ ì¬ë°°ì—´í•¨
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

permute_a = tensor_a.permute(0,2,1)
print(permute_a)
print('Shape: ', permute_a.size())
```

    tensor([[[3, 5, 1, 4, 4],
             [1, 5, 8, 2, 2]],
    
            [[9, 6, 3, 5, 6],
             [1, 6, 7, 5, 1]],
    
            [[3, 4, 6, 6, 6],
             [8, 2, 8, 4, 6]]])
    Shape:  torch.Size([3, 2, 5])
    
    
    tensor([[[3, 1],
             [5, 5],
             [1, 8],
             [4, 2],
             [4, 2]],
    
            [[9, 1],
             [6, 6],
             [3, 7],
             [5, 5],
             [6, 1]],
    
            [[3, 8],
             [4, 2],
             [6, 8],
             [6, 4],
             [6, 6]]])
    Shape:  torch.Size([3, 5, 2])
    

### 2.2 í…ì„œì˜ ì°¨ì›ì„ ì¶”ê°€í•˜ê±°ë‚˜ ë³€ê²½í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ


```python
tensor_a = torch.tensor([i for i in range(10)]).reshape(5,2)
print(tensor_a)
print('Shape: ', tensor_a.size())
print('\n')

unsqu_a = tensor_a.unsqueeze(0) # 0ë²ˆì§¸ ì°¨ì› í•˜ë‚˜ ì¶”ê°€ (5,2) => (1,5,2)
print(unsqu_a)
print('Shape: ', unsqu_a.size())
```

    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])
    Shape:  torch.Size([5, 2])
    
    
    tensor([[[0, 1],
             [2, 3],
             [4, 5],
             [6, 7],
             [8, 9]]])
    Shape:  torch.Size([1, 5, 2])
    


```python
unsqu_a2 = tensor_a.unsqueeze(-1) # ë§ˆì§€ë§‰ ë²ˆì§¸ì— ì°¨ì› í•˜ë‚˜ ì¶”ê°€ (5,2) => (5,2,1)
print(unsqu_a2)
print('Shape: ', unsqu_a2.size())
```

    tensor([[[0],
             [1]],
    
            [[2],
             [3]],
    
            [[4],
             [5]],
    
            [[6],
             [7]],
    
            [[8],
             [9]]])
    Shape:  torch.Size([5, 2, 1])
    


```python
# squeeze: í…ì„œì— ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì„ ì œê±°í•¨
print(unsqu_a)
print('Shape: ', unsqu_a.size())
print('\n')

squ = unsqu_a.squeeze()
print(squ)
print('Shape: ', squ.size())
```

    tensor([[[0, 1],
             [2, 3],
             [4, 5],
             [6, 7],
             [8, 9]]])
    Shape:  torch.Size([1, 5, 2])
    
    
    tensor([[0, 1],
            [2, 3],
            [4, 5],
            [6, 7],
            [8, 9]])
    Shape:  torch.Size([5, 2])
    


```python
x = torch.zeros(2,1,2,1,2)
print('Shape(original): ', x.size())
print('\n')

print('Shape(squeeze()): ', x.squeeze().size()) # ì°¨ì›ì´ 1ì¸ ëª¨ë“  ì°¨ì› ì œê±°
print('\n')

print('Shape(squeeze(0)): ', x.squeeze(0).size()) # 0ë²ˆì§¸ ì°¨ì›ì€ ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì´ ì•„ë‹ˆë¯€ë¡œ ë³€í™” ì—†ìŒ
print('\n')

print('Shape(squeeze(1)): ', x.squeeze(1).size()) # 1ë²ˆì§¸ ì°¨ì›ì€ ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì´ë¯€ë¡œ ì œê±°
print('\n')

print('Shape(squeeze(0,1,3)): ', x.squeeze((0,1,3)).size()) # ì—¬ëŸ¬ ì°¨ì› ì œê±° ê°€ëŠ¥
```

    Shape(original):  torch.Size([2, 1, 2, 1, 2])
    
    
    Shape(squeeze()):  torch.Size([2, 2, 2])
    
    
    Shape(squeeze(0)):  torch.Size([2, 1, 2, 1, 2])
    
    
    Shape(squeeze(1)):  torch.Size([2, 2, 1, 2])
    
    
    Shape(squeeze(0,1,3)):  torch.Size([2, 2, 2])
    

- Aí…ì„œê°€ 1ì°¨ì›ì¼ ê²½ìš°: Aí…ì„œì˜ í¬ê¸°ê°€ (m,)ì´ë©´ mì€ ê³ ì •í•˜ê³  (x,m)ì˜ í¬ê¸°ë¡œë§Œ í™•ì¥ ê°€ëŠ¥


```python
# expand: í…ì„œì˜ ê°’ì„ ë°˜ë³µí•˜ì—¬ í¬ê¸°ë¥¼ í™•ì¥í•¨
tensor_1dim = torch.tensor([1,2,3,4])
print(tensor_1dim)
print('Shape: ', tensor_1dim.size())
print('\n')

expand_tensor = tensor_1dim.expand(3,4)
print(expand_tensor)
print('Shape: ', expand_tensor.size())
```

    tensor([1, 2, 3, 4])
    Shape:  torch.Size([4])
    
    
    tensor([[1, 2, 3, 4],
            [1, 2, 3, 4],
            [1, 2, 3, 4]])
    Shape:  torch.Size([3, 4])
    

- Aí…ì„œê°€ 2ì°¨ì› ì´ìƒì¼ ê²½ìš°: í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì— ëŒ€í•´ì„œë§Œ ì ìš© ê°€ëŠ¥. Aí…ì„œì˜ í¬ê¸°ê°€ (1,m)ì´ë©´ (x,m), (m,1)ì´ë©´ (m,y)ë¡œë§Œ í™•ì¥ ê°€ëŠ¥


```python
tensor_2dim = torch.tensor([[1,2,3,4],[1,2,3,4]])
print(tensor_2dim)
print('Shape: ', tensor_2dim.size())
print('\n')

# ì´ë ‡ê²Œ í•˜ë©´ ì—ëŸ¬ ë°œìƒí•¨
expand_tensor = tensor_2dim.expand(4,4)
print(expand_tensor)
print('Shape: ', expand_tensor.size())
```

    tensor([[1, 2, 3, 4],
            [1, 2, 3, 4]])
    Shape:  torch.Size([2, 4])
    
    
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-51-fd068b76c84f> in <cell line: 7>()
          5 
          6 # ì´ë ‡ê²Œ í•˜ë©´ ì—ëŸ¬ ë°œìƒí•¨
    ----> 7 expand_tensor = tensor_2dim.expand(4,4)
          8 print(expand_tensor)
          9 print('Shape: ', expand_tensor.size())
    

    RuntimeError: The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [4, 4].  Tensor sizes: [2, 4]



```python
# repeat: í…ì„œë¥¼ ë°˜ë³µí•´ì„œ í¬ê¸°ë¥¼ í™•ì¥í•¨
# ex) Aí…ì„œê°€ (m,n)í¬ê¸°ì¼ ë•Œ Aí…ì„œë¥¼ repeat(i,j)í•˜ë©´ ê²°ê³¼ê°’ìœ¼ë¡œ (m*i, n*j)ì˜ í¬ê¸°ì˜ í…ì„œê°€ ìƒì„±ë¨
tensor_1dim = torch.tensor([1,2,3,4])
print(tensor_1dim)
print('Shape: ', tensor_1dim.size())
print('\n')

repeat_tensor = tensor_1dim.repeat(3,4)
print(repeat_tensor)
print('Shape: ', repeat_tensor.size())
```

    tensor([1, 2, 3, 4])
    Shape:  torch.Size([4])
    
    
    tensor([[1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],
            [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],
            [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]])
    Shape:  torch.Size([3, 16])
    


```python
# flatten: ë‹¤ì°¨ì› í…ì„œë¥¼ 1ì°¨ì› í…ì„œë¡œ ë³€ê²½í•¨
t = torch.tensor([i for i in range(20)]).reshape(2,5,2)
print(t)
print('Shape: ', t.size())
print('\n')

flat_tensor = t.flatten() # +) start_dimë„ ì„¤ì •í•´ì¤„ ìˆ˜ ìˆìŒ. ê¸°ë³¸ê°’ì€ 0ì„
print(flat_tensor)
print('Shape: ', flat_tensor.size())
```

    tensor([[[ 0,  1],
             [ 2,  3],
             [ 4,  5],
             [ 6,  7],
             [ 8,  9]],
    
            [[10, 11],
             [12, 13],
             [14, 15],
             [16, 17],
             [18, 19]]])
    Shape:  torch.Size([2, 5, 2])
    
    
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
            18, 19])
    Shape:  torch.Size([20])
    


```python
# ravel: ë‹¤ì°¨ì› í…ì„œë¥¼ 1ì°¨ì› í…ì„œë¡œ ë³€ê²½í•¨
t = torch.tensor([i for i in range(20)]).reshape(2,5,2)
print(t)
print('Shape: ', t.size())
print('\n')

ravel_tensor = t.ravel()
print(ravel_tensor)
print('Shape: ', ravel_tensor.size())
```

    tensor([[[ 0,  1],
             [ 2,  3],
             [ 4,  5],
             [ 6,  7],
             [ 8,  9]],
    
            [[10, 11],
             [12, 13],
             [14, 15],
             [16, 17],
             [18, 19]]])
    Shape:  torch.Size([2, 5, 2])
    
    
    tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
            18, 19])
    Shape:  torch.Size([20])
    


```python
t.ravel(1) # ì—ëŸ¬ ë°œìƒ, ravelì€ flattenê³¼ ë‹¬ë¦¬ ì–´ë– í•œ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ í‰íƒ„í™” í•˜ëŠ” ì‘ì—…ì´ ì—†ìŒ
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-55-3baa156994b8> in <cell line: 1>()
    ----> 1 t.ravel(1) # ì—ëŸ¬ ë°œìƒ, ravelì€ flattenê³¼ ë‹¬ë¦¬ ì–´ë– í•œ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ í‰íƒ„í™” í•˜ëŠ” ì‘ì—…ì´ ì—†ìŒ
    

    TypeError: _TensorBase.ravel() takes no arguments (1 given)


### 2.3 ì—­í• ì´ ë¹„ìŠ·í•œ í•¨ìˆ˜ë“¤ì˜ ì°¨ì´ ì´í•´ ë° ì‹¤ìŠµ

2.3.1 ëª¨ì–‘ ë³€ê²½: view vs reshape vs unsqueeze   
- contiguousë€?
  - í…ì„œì˜ ë©”ëª¨ë¦¬ ìƒì— ì—°ì†ì ì¸ ë°ì´í„° ë°°ì¹˜ë¥¼ ê°–ëŠ” ê²ƒ
  - í…ì„œë¥¼ ì²­ë¯€ ìƒì„± í›„ ì •ì˜í•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ contiguous í•˜ì§€ë§Œ, ì´ì— ëŒ€í•´ ì°¨ì›ì˜ ìˆœì„œë¥¼ ë³€ê²½í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹˜ë©´ contiguousí•˜ì§€ ì•Šì•„ì§
  - í…ì„œì˜ contiguousí•¨ì„ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” is_contiguous()ë¥¼ ì‚¬ìš©í•˜ë©´ ë¨

- view: contiguousí•˜ì§€ ì•Šì€ í…ì„œì— ëŒ€í•´ì„œ ë™ì‘í•˜ì§€ ì•ŠìŒ
- reshape: contiguousí•˜ì§€ ì•Šì€ í…ì„œë¥¼ contiguousí•˜ê²Œ ë§Œë“¤ì–´ì£¼ê³  í¬ê¸°ë¥¼ ë³€ê²½í•¨
- unsqueeze: ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì¸ ì°¨ì›ì„ ì¶”ê°€í•˜ì§€ë§Œ ì°¨ì›ì˜ í¬ê¸°ê°€ 1ì´ ì•„ë‹ˆë©´ ì°¨ì›ì˜ ëª¨ì–‘ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŒ


```python
# view vs reshape
tmp = torch.tensor([[[0,1],[2,3],[4,5]],
                    [[6,7],[8,9],[10,11]],
                    [[12,13],[14,15],[16,17]],
                    [[18,19],[20,21],[22,23]]])
tmp_t = tmp.transpose(0,1)
print(tmp_t.is_contiguous())
print(tmp_t.view(-1)) # ì—ëŸ¬ë°œìƒ, viewëŠ” contiguous í•˜ì§€ ì•Šì€ í…ì„œì— ëŒ€í•´ì„  ë™ì‘ì´ ë˜ì§€ ì•ŠìŒ
```

    False
    


    ---------------------------------------------------------------------------

    RuntimeError                              Traceback (most recent call last)

    <ipython-input-56-c191427fbaf7> in <cell line: 5>()
          3 tmp_t = tmp.transpose(0,1)
          4 print(tmp_t.is_contiguous())
    ----> 5 print(tmp_t.view(-1))
    

    RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.



```python
reshape_tmp = tmp_t.reshape(-1) # reshapeì€ contiguousí•˜ì§€ ì•Šì•„ë„ ë™ì‘ì´ ë¨
print(reshape_tmp)
print(reshape_tmp.is_contiguous()) # contiguousí•˜ì§€ ì•Šì•˜ë˜ Tensorë¥¼ contiguousí•˜ê²Œ ë³€ê²½í•´ì¤Œ
```

    tensor([ 0,  1,  6,  7, 12, 13, 18, 19,  2,  3,  8,  9, 14, 15, 20, 21,  4,  5,
            10, 11, 16, 17, 22, 23])
    True
    


```python
# (view, reshape) vs unsqueeze
tensor_a = torch.randn(2,3)

# (2,3)ì˜ í…ì„œë¥¼ (2,3,1)ì˜ í¬ê¸°ë¡œ ë³€ê²½
view_tensor = tensor_a.view(2,3,1)
reshape_tensor = tensor_a.reshape(2,3,1)
unsqueeze_tensor = tensor_a.unsqueeze(-1) # unsqueezeëŠ” ë¹„ì–´ìˆëŠ” ì°¨ì›ì— í•˜ë‚˜ë¥¼ ì¶”ê°€í•˜ëŠ” ëª¨ì–‘ë³€ê²½ì´ì§€ ë‹¤ë¥¸ ëª¨ì–‘ë³€ê²½ì„ í•  ìˆœ ì—†ë‹¤ ì •ë„ë¡œ ì´í•´í•˜ë©´ ë¨

print('View output size: ', view_tensor.size())
print('Reshape output size: ', reshape_tensor.size())
print('Unsqueeze output size: ', unsqueeze_tensor.size())
```

    View output size:  torch.Size([2, 3, 1])
    Reshape output size:  torch.Size([2, 3, 1])
    Unsqueeze output size:  torch.Size([2, 3, 1])
    

2.3.2 ì°¨ì›ë³€ê²½: transpose vs permute
- transpose: ë‘ ì°¨ì›ì— ëŒ€í•´ì„œë§Œ ë³€ê²½ ê°€ëŠ¥í•¨
  - ì¸ìê°€ ì´ 2ê°œì—¬ì•¼ í•¨
- permute: ëª¨ë“  ì°¨ì›ì— ëŒ€í•´ì„œ ë³€ê²½ ê°€ëŠ¥í•¨
  - ì¸ìê°€ ì°¨ì›ì˜ ê°œìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨


```python
import torch
tensor_a = torch.randn(2,3,2)
transpose_tensor = tensor_a.transpose(2,1) # í–‰ê³¼ ì—´ì„ ì „ì¹˜
permute_tensor = tensor_a.permute(0,2,1) # í–‰ê³¼ ì—´ì„ ë°”ê¿ˆ

print('Transpose tensor shape: ', transpose_tensor.size())
print('Permute tensor shape: ', permute_tensor.size())
```

    Transpose tensor shape:  torch.Size([2, 2, 3])
    Permute tensor shape:  torch.Size([2, 2, 3])
    


```python
tensor_a.permute(1,2,0).shape # permuteëŠ” ì´ë ‡ê²Œë„ ê°€ëŠ¥í•¨
```




    torch.Size([3, 2, 2])




```python
tensor_a.transpose(2,1).transpose(0,2).shape # transposeëŠ” ì´ë ‡ê²Œ ë‘ ë²ˆì— ê±¸ì³ì„œ í•´ì•¼ í•¨
```




    torch.Size([3, 2, 2])



2.3.3 ë°˜ë³µì„ í†µí•œ í…ì„œ í¬ê¸° í™•ì¥: expand vs repeat
- expand
  - ì›ë³¸ í…ì„œì™€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•¨
- repeat
  - ì›ë³¸ í…ì„œì™€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ì§€ ì•ŠìŒ


```python
# ì›ë³¸ í…ì„œ ìƒì„±
tensor_a = torch.rand(1,1,3)
print('Original Tensor Size')
print(tensor_a.size())
print(tensor_a)
print('\n')

# expand ì‚¬ìš©í•˜ì—¬ (1,1,3) -> (4,1,3)
expand_tensor = tensor_a.expand(4,1,-1)
print('Shape of expanded tensor: ', expand_tensor.size())
print('\n')

# repeat ì‚¬ìš©í•˜ì—¬ (1,1,3) -> (4,1,3)
repeat_tensor = tensor_a.repeat(4,1,1)
print('Shape of repeated tensor: ', repeat_tensor.size())
print('\n')

# í‰ë©´í™”ëœ ë·° ìˆ˜ì • í›„ ì›ë³¸ í…ì„œ í™•ì¸
tensor_a[:] = 0

print('Expanded Tensor')
print(expand_tensor) # ê°’ ë³€ê²½ ë¨
print('\n')

print('Repeated Tensor')
print(repeat_tensor) # ê°’ ë³€ê²½ ì•ˆ ë¨
```

    Original Tensor Size
    torch.Size([1, 1, 3])
    tensor([[[0.6953, 0.8855, 0.1160]]])
    
    
    Shape of expanded tensor:  torch.Size([4, 1, 3])
    
    
    Shape of repeated tensor:  torch.Size([4, 1, 3])
    
    
    Expanded Tensor
    tensor([[[0., 0., 0.]],
    
            [[0., 0., 0.]],
    
            [[0., 0., 0.]],
    
            [[0., 0., 0.]]])
    
    
    Repeated Tensor
    tensor([[[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]],
    
            [[0.6953, 0.8855, 0.1160]]])
    

## 3. í…ì„œ í•©ì¹˜ê¸°ì™€ ë‚˜ëˆ„ê¸°

### 3.1 ì—¬ëŸ¬ í…ì„œë¥¼ í•©ì¹˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ


```python
# cat: ì£¼ì–´ì§„ ì°¨ì›ì„ ë”°ë¼ í…ì„œë“¤ì„ ì—°ê²°í•¨ (ì£¼ì–´ì§„ ì°¨ì› ì™¸ì˜ ë‹¤ë¥¸ ì°¨ì›ì˜ í¬ê¸°ê°€ ê°™ì•„ì•¼ í•¨)
tensor_a = torch.randint(1,10,(2,3))
tensor_b = torch.rand(5,3)

print('Tensor A shape: ', tensor_a.size())
print(tensor_a)
print('\n')

print('Tensor B shape: ', tensor_b.size())
print(tensor_b)
print('\n')

a_cat_b_row = torch.cat((tensor_a, tensor_b), dim=0) # dim=0 (í–‰)
print('Concat Tensor A and B (by row) Shape: ', a_cat_b_row.shape) # (TensorAì˜ í–‰ ê°œìˆ˜ + TensorBì˜ í–‰ ê°œìˆ˜, Tensor A/Bì˜ ì—´ ê°œìˆ˜)
print(a_cat_b_row)
```

    Tensor A shape:  torch.Size([2, 3])
    tensor([[2, 3, 9],
            [6, 9, 7]])
    
    
    Tensor B shape:  torch.Size([5, 3])
    tensor([[0.6792, 0.6251, 0.3015],
            [0.9122, 0.3750, 0.3705],
            [0.2549, 0.9658, 0.6880],
            [0.4726, 0.5449, 0.4260],
            [0.4259, 0.7678, 0.3151]])
    
    
    Concat Tensor A and B (by row) Shape:  torch.Size([7, 3])
    tensor([[2.0000, 3.0000, 9.0000],
            [6.0000, 9.0000, 7.0000],
            [0.6792, 0.6251, 0.3015],
            [0.9122, 0.3750, 0.3705],
            [0.2549, 0.9658, 0.6880],
            [0.4726, 0.5449, 0.4260],
            [0.4259, 0.7678, 0.3151]])
    


```python
# stack: ì£¼ì–´ì§„ ì°¨ì›ì„ ìƒˆë¡œìš´ ì°¨ì›ìœ¼ë¡œ ì¶”ê°€í•˜ì—¬ í…ì„œë“¤ì„ ìŒ“ìŒ
tensor_a = torch.randint(1,10,(3,2))
tensor_b = torch.rand(3,2)

print('Tensor A shape: ', tensor_a.size())
print(tensor_a)
print('\n')

print('Tensor B shape: ', tensor_b.size())
print(tensor_b)
print('\n')

stack_tensor_row = torch.stack([tensor_a, tensor_b], dim=0) # dim=0, í–‰ì„ ê¸°ì¤€ìœ¼ë¡œ TensorAì— TensorB ìŒ“ê¸°
print('Stack A and B (by row): ', stack_tensor_row.size()) # ìŒ“ì€ Tensor ê°œìˆ˜, Tensor A/B í–‰ ê³„ìˆ˜, Tensor A/B ì—´ ê°œìˆ˜)
print(stack_tensor_row)
```

    Tensor A shape:  torch.Size([3, 2])
    tensor([[3, 2],
            [9, 9],
            [6, 8]])
    
    
    Tensor B shape:  torch.Size([3, 2])
    tensor([[0.3521, 0.6629],
            [0.2885, 0.1568],
            [0.0380, 0.5313]])
    
    
    Stack A and B (by row):  torch.Size([2, 3, 2])
    tensor([[[3.0000, 2.0000],
             [9.0000, 9.0000],
             [6.0000, 8.0000]],
    
            [[0.3521, 0.6629],
             [0.2885, 0.1568],
             [0.0380, 0.5313]]])
    

### 3.2 í•˜ë‚˜ì˜ í…ì„œë¥¼ ì—¬ëŸ¬ê°œë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì— ëŒ€í•œ ì´í•´ ë° ì‹¤ìŠµ


```python
# chunk: ë‚˜ëˆˆê³ ì í•˜ëŠ” í…ì„œì˜ ê°œìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì›ë˜ì˜ í…ì„œë¥¼ ê°œìˆ˜ì— ë§ê²Œ ë¶„ë¦¬í•¨
tensor_a = torch.randint(1,10,(6,4))
print('Original: ', tensor_a)
print('\n')

chunk_num = 3
chunk_tensor = torch.chunk(tensor_a, chunks = chunk_num, dim=0)
print(f'{len(chunk_tensor)} ê°œì˜ Tensorë¡œ ë¶„ë¦¬')
print('\n')

for idx,a in enumerate(chunk_tensor):
  print(f'{idx}ë²ˆì§¸ Tensor \n{a}')
  print(f'{idx}ë²ˆì§¸ Tensor í¬ê¸°', a.size())
  print('----' * 10)
```

    Original:  tensor([[4, 4, 1, 4],
            [5, 2, 4, 7],
            [6, 6, 6, 1],
            [7, 9, 2, 1],
            [7, 8, 4, 8],
            [4, 5, 7, 8]])
    
    
    3 ê°œì˜ Tensorë¡œ ë¶„ë¦¬
    
    
    0ë²ˆì§¸ Tensor 
    tensor([[4, 4, 1, 4],
            [5, 2, 4, 7]])
    0ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    1ë²ˆì§¸ Tensor 
    tensor([[6, 6, 6, 1],
            [7, 9, 2, 1]])
    1ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    2ë²ˆì§¸ Tensor 
    tensor([[7, 8, 4, 8],
            [4, 5, 7, 8]])
    2ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    


```python
# split: ì…ë ¥í•œ í¬ê¸°ë¡œ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ í…ì„œë¡œ ë‚˜ëˆ”
tensor_a = torch.randint(1,10,(6,4))
print(tensor_a)
print('\n')

split_size = 2
split_tensor = torch.split(tensor_a, split_size_or_sections = split_size, dim = 0) # dim=0(í–‰), í…ì„œAë¥¼ í–‰ì˜ ê¸¸ì´ê°€ 2(split_size)ì¸ í…ì„œë¡œ ë‚˜ëˆ”
print(f'{len(split_tensor)}ê°œì˜ Tensorë¡œ ë¶„ë¦¬')
print('\n')

for idx,a in enumerate(split_tensor):
  print(f'{idx}ë²ˆì§¸ Tensor \n{a}')
  print(f'{idx}ë²ˆì§¸ Tensor í¬ê¸°', a.size())
  print('----' * 10)
```

    tensor([[4, 4, 9, 6],
            [2, 7, 1, 7],
            [3, 5, 9, 4],
            [7, 5, 8, 3],
            [8, 4, 1, 2],
            [7, 9, 8, 8]])
    
    
    3ê°œì˜ Tensorë¡œ ë¶„ë¦¬
    
    
    0ë²ˆì§¸ Tensor 
    tensor([[4, 4, 9, 6],
            [2, 7, 1, 7]])
    0ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    1ë²ˆì§¸ Tensor 
    tensor([[3, 5, 9, 4],
            [7, 5, 8, 3]])
    1ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    2ë²ˆì§¸ Tensor 
    tensor([[8, 4, 1, 2],
            [7, 9, 8, 8]])
    2ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    


```python
# split_sizeì˜ ì…ë ¥ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„£ì„ ìˆ˜ë„ ìˆìŒ
tensor_a = torch.randint(1,10,(6,4))
print('Original: ', tensor_a)
print('\n')

split_size = [2,4]
split_tensor = torch.split(tensor_a, split_size_or_sections = split_size, dim = 0) # dim=0(í–‰), í…ì„œAë¥¼ í–‰ì˜ ê¸¸ì´ê°€ 2(split_size)ì¸ í…ì„œë¡œ ë‚˜ëˆ”
print(f'{len(split_tensor)}ê°œì˜ Tensorë¡œ ë¶„ë¦¬')
print('\n')

for idx,a in enumerate(split_tensor):
  print(f'{idx}ë²ˆì§¸ Tensor \n{a}')
  print(f'{idx}ë²ˆì§¸ Tensor í¬ê¸°', a.size())
  print('----' * 10)
```

    Original:  tensor([[7, 1, 7, 6],
            [5, 9, 4, 6],
            [6, 9, 6, 6],
            [3, 4, 7, 9],
            [7, 5, 7, 5],
            [2, 8, 8, 9]])
    
    
    2ê°œì˜ Tensorë¡œ ë¶„ë¦¬
    
    
    0ë²ˆì§¸ Tensor 
    tensor([[7, 1, 7, 6],
            [5, 9, 4, 6]])
    0ë²ˆì§¸ Tensor í¬ê¸° torch.Size([2, 4])
    ----------------------------------------
    1ë²ˆì§¸ Tensor 
    tensor([[6, 9, 6, 6],
            [3, 4, 7, 9],
            [7, 5, 7, 5],
            [2, 8, 8, 9]])
    1ë²ˆì§¸ Tensor í¬ê¸° torch.Size([4, 4])
    ----------------------------------------
    
