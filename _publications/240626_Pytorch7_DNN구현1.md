---
layout: post
title: "240626(ìˆ˜) [ì˜¨ë¼ì¸ê°•ì˜] Pytorch7_DNNêµ¬í˜„1"
subtitle: "[Tips]"
date: 2024-06-26 22:10
background: 
tag: [Tips, Github io, Notion]
---

# [ì˜¨ë¼ì¸ê°•ì˜]Pytorch7_DNNêµ¬í˜„1

### **~ ëª©ì°¨ ~**
1. ë°ì´í„°   
  1.1 torchvision ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ MNIST Dataset ë¶ˆëŸ¬ì˜¤ê¸°   
  1.2 ë¶ˆëŸ¬ì˜¨ Datasetë¥¼ ì‚¬ìš©í•´ì„œ DataLoaderë¥¼ ì •ì˜í•˜ê³  DataLoaderì˜ ì¸ìì— ëŒ€í•œ ì´í•´
2. ëª¨ë¸   
  2.1 nn.Moduleì„ ì‚¬ìš©í•´ì„œ Custom model ì •ì˜   
  2.2 ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”    
3. ì „ì²´ ì½”ë“œ

# 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸, ì‹œë“œê³ ì •


```python
!pip install torch==2.0.1 -q
!pip install torchvision==0.15.2 -q
```

    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m619.9/619.9 MB[0m [31m2.1 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m21.0/21.0 MB[0m [31m56.8 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m849.3/849.3 kB[0m [31m48.4 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.8/11.8 MB[0m [31m73.8 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m557.1/557.1 MB[0m [31m2.2 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m317.1/317.1 MB[0m [31m2.0 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m168.4/168.4 MB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m54.6/54.6 MB[0m [31m10.9 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m102.6/102.6 MB[0m [31m9.0 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m173.2/173.2 MB[0m [31m3.4 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m177.1/177.1 MB[0m [31m6.7 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m98.6/98.6 kB[0m [31m11.0 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m63.3/63.3 MB[0m [31m9.9 MB/s[0m eta [36m0:00:00[0m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m96.4/96.4 kB[0m [31m11.3 MB/s[0m eta [36m0:00:00[0m
    [?25h[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.
    torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.0.1 which is incompatible.
    torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.0.1 which is incompatible.[0m[31m
    [2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.0/6.0 MB[0m [31m12.0 MB/s[0m eta [36m0:00:00[0m
    [?25h


```python
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader # Optimizer ì„¤ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

import torchvision # PyTorchì˜ ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬
import torchvision.transforms as T # ì´ë¯¸ì§€ ë³€í™˜ì„ ìœ„í•œ ëª¨ë“ˆ
import torchvision.utils as vutils # ì´ë¯¸ì§€ë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
```

í•­ìƒ ì¼ê´€ëœ ì‹¤í—˜ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ ì‹œë“œê³ ì •ì„ í•´ì¤Œ


```python
# seed ê³ ì •
import random # íŒŒì´ì¬ì˜ ê¸°ë³¸ ë‚œìˆ˜ìƒì„±ê¸°
import torch.backends.cudnn as cudnn # PyTorchì˜ CUDA ë²„ì „ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒí‚¤ê¸°ìœ„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” CuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬

def random_seed(seed_num):

  # PyTorchì˜ ë‚œìˆ˜ ìƒì„±ê¸° ê³ ì •
  torch.manual_seed(seed_num)

  # PyTorchì˜ CUDAì—°ì‚°ì„ ìœ„í•œ ë‚œìˆ˜ ìƒì„±ê¸° ê³ ì •
  torch.cuda.manual_seed(seed_num)

  # ëª¨ë“  GPUì—ì„œ ë™ì¼í•œ seed ê°’ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
  torch.cuda.manual_seed_all(seed_num)

  # CuDNNì´ ê°€ì¥ ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ê¸° ìœ„í•´ ë²¤ì¹˜ë§ˆí‚¹í•˜ëŠ” ê²ƒì„ ë¹„í™œì„±í™”í•¨. ì´ëŠ” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ í•„ìš”í•¨
  cudnn.benchmark = False

  # CuDNNì˜ ê²°ì •ë¡ ì  ë™ì‘(ë™ì¼í•œ ì‘ì—…ì„ ë°˜ë³µí•  ë•Œ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ëŠ” ê²ƒ)ì„ í™œì„±í™”í•˜ì—¬ ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê²Œ í•¨
  cudnn.deterministic = True

  random.seed(seed_num)

random_seed(42)
```

## 1. ë°ì´í„°

### 1.1 torchvision ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ MNIST Dataset ë¶ˆëŸ¬ì˜¤ê¸°   



```python
# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ í•„ìš”í•œ ë³€í™˜(transform)ì„ ì •ì˜í•¨
mnist_transform = T.Compose([
    T.ToTensor() # í…ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
])
```


```python
# torchvision ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ MNIST Datasetì„ ë¶ˆëŸ¬ì˜´
download_root = './MNIST_DATASET'

train_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train=True, download=True)
test_dataset = torchvision.datasets.MNIST(download_root, transform=mnist_transform, train=False, download=True)
```

    Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden
    
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz
    

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9912422/9912422 [00:01<00:00, 6042399.00it/s]
    

    Extracting ./MNIST_DATASET/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_DATASET/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden
    
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz
    

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28881/28881 [00:00<00:00, 157966.54it/s]
    

    Extracting ./MNIST_DATASET/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_DATASET/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden
    
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz
    

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1648877/1648877 [00:01<00:00, 1287927.30it/s]
    

    Extracting ./MNIST_DATASET/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_DATASET/MNIST/raw
    
    Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
    Failed to download (trying next):
    HTTP Error 403: Forbidden
    
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
    Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz
    

    100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4542/4542 [00:00<00:00, 3148327.35it/s]

    Extracting ./MNIST_DATASET/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_DATASET/MNIST/raw
    
    

    
    


```python
for image, label in train_dataset:
  print(image.shape, label)
  break
```

    torch.Size([1, 28, 28]) 5
    


```python
# ë°ì´í„°ì…‹ì„ í•˜ìŠµ ë°ì´í„°ì…‹ê³¼ ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•¨
total_size = len(train_dataset)
train_num, valid_num = int(total_size * 0.8), int(total_size * 0.2)
print('Train dataset ê°œìˆ˜: ', train_num)
print('Test dataset ê°œìˆ˜: ', valid_num)
train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_num, valid_num])
```

    Train dataset ê°œìˆ˜:  48000
    Test dataset ê°œìˆ˜:  12000
    

###1.2 ë¶ˆëŸ¬ì˜¨ Datasetë¥¼ ì‚¬ìš©í•´ì„œ DataLoaderë¥¼ ì •ì˜í•˜ê³  DataLoaderì˜ ì¸ìì— ëŒ€í•œ ì´í•´

- DataLoaderëŠ” ì¸ìë¡œ ì£¼ì–´ì§„ Datasetì„ ì´ìš©í•´ì„œ ë‹¨ì¼ ë°ì´í„°ë“¤ì„ ì •í•´ì§„ ê°œìˆ˜ë§Œí¼ ëª¨ì•„ ë¯¸ë‹ˆë°°ì¹˜(mini-batch)ë¥¼ êµ¬ì„±í•˜ëŠ” ì—­í• ì„ í•¨. `torch.utils.data`ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì”€


```python
batch_size = 32

# ì•ì„œ ì„ ì–¸í•œ Datasetì„ ì¸ìë¡œ ì£¼ì–´ DataLoaderë¥¼ ì„ ì–¸í•¨
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # shuffle=True: ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œ ì—í¬í¬ë§ˆë‹¤ ë°ì´í„°ê°€ ì„ì—¬ì•¼ ì¼ë°˜í™”ëœ í•™ìŠµì´ ê°€ëŠ¥í•¨
valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) # shuffle=False: ì¼ê´€ëœ ìˆœì„œë¡œ ì œê³µë˜ì–´ì•¼ ë©” ì—í¬í¬ë§ˆë‹¤ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŒ
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # shuffle=False: ìˆœì„œê°€ ì¼ì •í•´ì•¼ ë¹„êµ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
```

- ì•ì—ì„œëŠ” image.shapeê°€ [1,28,28]ì˜€ëŠ”ë° ì§€ê¸ˆì€ [32,1,28,28]ì„. ì´ëŠ” 32ê°œì˜ ì†ê¸€ì”¨ ë°ì´í„°ê°€ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë¬¶ì—¬ìˆë‹¤ëŠ” ëœ»ì„


```python
# í•™ìŠµ ë°ì´í„°ë¡œë”ì—ì„œ ì¼ë¶€ì˜ ë¯¸ë‹ˆë°°ì¹˜ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
for images, labels in train_dataloader:
  print(images.shape, labels.shape)
  break
```

    torch.Size([32, 1, 28, 28]) torch.Size([32])
    


```python
grid = vutils.make_grid(images, nrow=8) # ê° í–‰ë§ˆë‹¤ 8ê°œì˜ ë¯¸ì§€ë¥¼ ë°°ì¹˜í•´ì„œ ê²©ìë¡œ êµ¬ì„±í•¨

# í•™ìŠµ ë°ì´í„°ë¡œë”ë¡œë¶€í„° ë¶ˆëŸ¬ì˜¨ ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”í•¨
plt.figure(figsize=(12,12))
plt.imshow(grid.numpy().transpose(1,2,0))
plt.title('mini batch visualization')
plt.axis('off')
plt.show()
```


    
![png](240626_Pytorch7_DNN%EA%B5%AC%ED%98%841_files/240626_Pytorch7_DNN%EA%B5%AC%ED%98%841_18_0.png)
    


## 2. ëª¨ë¸

###2.1 nn.Moduleì„ ì‚¬ìš©í•´ì„œ Custom model ì •ì˜   

- CNNì„ ì‚¬ìš©í•˜ê¸° ì „ì— ì´ë²ˆì—ëŠ” FCë ˆì´ì–´ë¡œë§Œ êµ¬ì„±ëœ DNN(Depp Neural Network)ì„ ë¨¼ì € êµ¬í˜„í•´ë³¼ê±°ì„

```
class DNN(nn.Module):
  def __init__(self, hidden_dims, num_classes, dropout_ratio, apply_batchnorm, apply_dropout, apply_Activation, set_super):
    if set_super:
      super().__init__()
    # FC ë ˆì´ì–´ë¥¼ ì„ ì–¸í•¨
    self.fc1 = nn.Linear(28*28, hidden_dim*4)
    self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)
    self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)
    self.classifier = nn.Linear(hidden_dim, num_classes)

    # Batch normalizationì„ ì„ ì–¸í•¨. apply_batchnorm ì¸ìê°€ Falseì¼ ê²½ìš°, batch normalizationì€ ì ìš©ë˜ì§€ ì•ŠìŒ
    # nn.Identity(): ì•„ë¬´ëŸ° ë³€í™˜ë„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ” ë ˆì´ì–´ì„. ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥
    self.batchnorm1 = nn.BatchNorm1d(hidden_dim*4) if apply_batchnorm else nn.Identity()
    self.batchnorm2 = nn.BatchNorm1d(hidden_dim*2) if apply_batchnorm else nn.Identity()
    self.batchnorm3 = nn.BatchNorm1d(hidden_dim) if apply_batchnorm else nn.Identity()

    # Dropoutì€ ë ˆì´ì–´ë¥¼ í†µê³¼í•œ ì¤‘ê°„ ì—°ì‚° ê²°ê³¼ë¥¼ dropout_ratioë§Œí¼ì˜ ë¹„ìœ¨ë¡œ elementë¥¼ 0ìœ¼ë¡œ ë³€ê²½í•¨
    # Dropoutì„ ì„ ì–¸í•¨. apply_dropout ì¸ìê°€ Falseì¼ ê²½ìš° dropoutì€ ì ìš©ë˜ì§€ ì•ŠìŒ
    self.dropout1 = nn.Dropout(dropout_ratio) if apply_dropout else nn.Identity()
    self.dropout2 = nn.Dropout(dropout_ratio) if apply_dropout else nn.Identity()
    self.dropout3 = nn.Dropout(dropout_ratio) if apply_dropout else nn.Identity()

    # Activation functionì„ ì„ ì–¸í•¨. apply_activation ì¸ìê°€ Falseì¼ ê²½ìš° activation functionì€ ì ìš©ë˜ì§€ ì•ŠìŒ
    self.activation1 = nn.ReLU() if apply_activation else nn.Identity()
    self.activation2 = nn.ReLU() if apply_activation else nn.Identity()
    self.activation3 = nn.ReLU() if apply_activation else nn.Identity()

    self.softmax = nn.LogSoftmax(dim=1) # LogSoftmax: ê·¸ëƒ¥ softmax í•¨ìˆ˜ë³´ë‹¤ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì  ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

  def forward(self,x):
    '''
    Input:
      x : [batch_size, 1, 28, 28]
    Output:
      output: [batch_size, num_classes]
    '''

    x = x.view(x.shape[0],-1)

    x = self.fc1(x) # [batch_size, dim*4]
    x = self.batchnorm1(x)
    x = self.activation1(x)
    x = self.dropout1(x)

    x = self.fc2(x) # [batch_size, dim*2]
    x = self.batchnorm2(x)
    x = self.activation2(x)
    x = self.dropout2(x)

    x = self.fc3(x) # [batch_size, dim]
    x = self.batchnorm3(x)
    x = self.activation3(x)
    x = self.dropout3(x)

    x = self.classifier(x) # [batch_size, 10]
    output = self.softmax(x)
    return output
```

ì•„ë˜ëŠ” ìœ„ì˜ ì½”ë“œë¥¼ ì¡°ê¸ˆ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì ì€ ë²„ì „ì„


```python
class DNN(nn.Module):
  def __init__(self, hidden_dims, num_classes, dropout_ratio, apply_batchnorm, apply_dropout, apply_activation, set_super):
    if set_super:
      super().__init__()

    self.hidden_dims = hidden_dims
    self.layers = nn.ModuleList()

    for i in range(len(self.hidden_dims)-1):
      self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))

      if apply_batchnorm:
        self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))

      if apply_activation:
        self.layers.append(nn.ReLU())

      if apply_dropout:
        self.layers.append(nn.Dropout(dropout_ratio))

    self.classifier = nn.Linear(self.hidden_dims[-1], num_classes)
    self.softmax = nn.LogSoftmax(dim=1)

  def forward(self,x):
    '''
    Input:
      x : [batch_size, 1, 28, 28]
    Output:
      output: [batch_size, num_classes]
    '''

    x = x.view(x.shape[0],-1)

    for layer in self.layers:
      x = layer(x)

    x = self.classifier(x)
    output = self.softmax(x)
    return output
```


```python
# ëª¨ë¸ì„ ì„ ì–¸í•œ í›„ MNIST ìˆ«ì ì´ë¯¸ì§€ ë°ì´í„°ì˜¤ ë™ì¼í•œ í¬ê¸°ì˜ random Tensorë¥¼ ì…ë ¥ìœ¼ë¡œë„£ì–´ ì—°ì‚°ìƒ ë¬¸ì œê°€ ì—†ëŠ”ì§€ í™•ì¸í•¨
hidden_dim = 128
hidden_dims = [784, hidden_dim * 4, hidden_dim * 2, hidden_dim]
model = DNN(hidden_dims, num_classes=10, dropout_ratio=0.2, apply_batchnorm=True, apply_dropout=True, apply_activation=True, set_super=True)
output = model(torch.randn((32,1,28,28)))
```

- ë§Œì•½ì— nn.Moduleì„ ë¨¼ì € ì´ˆê¸°í™”í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•¨


```python
model = DNN(hidden_dims, num_classes=10, dropout_ratio=0.2, apply_batchnorm=True, apply_dropout=True, apply_activation=True, set_super=False)
```


    ---------------------------------------------------------------------------

    AttributeError                            Traceback (most recent call last)

    <ipython-input-20-ac88750ee4ad> in <cell line: 1>()
    ----> 1 model = DNN(hidden_dims, num_classes=10, dropout_ratio=0.2, apply_batchnorm=True, apply_dropout=True, apply_activation=True, set_super=False)
    

    <ipython-input-17-c90092a68599> in __init__(self, hidden_dim, num_classes, dropout_ratio, apply_batchnorm, apply_dropout, apply_activation, set_super)
          5 
          6     self.hidden_dims = hidden_dims
    ----> 7     self.layers = nn.ModuleList()
          8 
          9     for i in range(len(self.hidden_dims)-1):
    

    /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in __setattr__(self, name, value)
       1641             if isinstance(value, Module):
       1642                 if modules is None:
    -> 1643                     raise AttributeError(
       1644                         "cannot assign module before Module.__init__() call")
       1645                 remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)
    

    AttributeError: cannot assign module before Module.__init__() call


###2.2 ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”


```python
# ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ”í•¨ìˆ˜
def weight_initialization(model, weight_init_method):
  for m in model.modules():
    if isinstance(m, nn.Linear):
      if weight_init_method == 'gaussian':
        nn.init.normal_(m.weight)
      elif weight_init_method == 'xavier':
        nn.init.xavier_normal_(m.weight)
      elif weight_init_method == 'kaiming':
        nn.init.kaiming_normal_(m.weight)
      elif weight_init_method == 'zeros':
        nn.init.zeros_(m.weight)

      nn.init.zeros_(m.bias)

  return model
```


```python
init_method = 'zeros' # gaussian, xavier, kaiming, zeros
model = weight_initialization(model, init_method)

for m in model.modules():
  if isinstance(m, nn.Linear):
    print(m.weight.data)
    break
```

    tensor([[0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            ...,
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.],
            [0., 0., 0.,  ..., 0., 0., 0.]])
    

## 3. ì „ì²´ ì½”ë“œ

- ê²°ë¡ ì ìœ¼ë¡œ ì „ì²´ ì½”ë“œëŠ” ì´ë ‡ê²Œ ë¨


```python
class DNN(nn.Module):
  def __init__(self, hidden_dims, num_classes, dropout_ratio, apply_batchnorm, apply_dropout, apply_activation, set_super):
    if set_super:
      super().__init__()

    self.hidden_dims = hidden_dims
    self.layers = nn.ModuleList()

    for i in range(len(self.hidden_dims)-1):
      self.layers.append(nn.Linear(self.hidden_dims[i], self.hidden_dims[i+1]))

      if apply_batchnorm:
        self.layers.append(nn.BatchNorm1d(self.hidden_dims[i+1]))

      if apply_activation:
        self.layers.append(nn.ReLU())

      if apply_dropout:
        self.layers.append(nn.Dropout(dropout_ratio))

    self.classifier = nn.Linear(self.hidden_dims[-1], num_classes)
    self.softmax = nn.LogSoftmax(dim=1)

  def forward(self,x):
    '''
    Input:
      x : [batch_size, 1, 28, 28]
    Output:
      output: [batch_size, num_classes]
    '''

    x = x.view(x.shape[0],-1)

    for layer in self.layers:
      x = layer(x)

    x = self.classifier(x)
    output = self.softmax(x)
    return output

  def weight_initialization(self, weight_init_method):
    for m in model.modules():
      if isinstance(m, nn.Linear):
        if weight_init_method == 'gaussian':
          nn.init.normal_(m.weight)
        elif weight_init_method == 'xavier':
          nn.init.xavier_normal_(m.weight)
        elif weight_init_method == 'kaiming':
          nn.init.kaiming_normal_(m.weight)
        elif weight_init_method == 'zeros':
          nn.init.zeros_(m.weight)

        nn.init.zeros_(m.bias)

  def count_parameters(self):
    return sum(p.numel() for p in self.parameters() if p.requires_grad) # numel(): í…ì„œì˜ ì›ì†Œ ê°œìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
```


```python
model = DNN(hidden_dims, num_classes=10, dropout_ratio=0.2, apply_batchnorm=True, apply_dropout=True, apply_activation=True, set_super=True)
init_method = 'gaussian'
model.weight_initialization(init_method)
```


```python
print(f'The model has {model.count_parameters():,} trainable parameters')
```

    The model has 569,226 trainable parameters
    
